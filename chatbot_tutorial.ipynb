{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_name = \"cornell movie-dialogs corpus\"\n",
    "corpus = os.path.join(\"data\", corpus_name)\n",
    "\n",
    "def printLines(file, n=10):\n",
    "    with open(file, 'rb') as datafile:\n",
    "        lines = datafile.readlines()\n",
    "    for line in lines[:n]:\n",
    "        print(line)\n",
    "\n",
    "# printLines(os.path.join(corpus, \"movie_lines.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits each line of the file into a dictionary of fields\n",
    "def loadLines(fileName, fields):\n",
    "    lines = {}\n",
    "    with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            values = line.split(\" +++$+++ \")\n",
    "            # Extract fields\n",
    "            lineObj = {}\n",
    "            for i, field in enumerate(fields):\n",
    "                lineObj[field] = values[i]\n",
    "            lines[lineObj['lineID']] = lineObj\n",
    "    return lines\n",
    "\n",
    "\n",
    "# Groups fields of lines from `loadLines` into conversations based on *movie_conversations.txt*\n",
    "def loadConversations(fileName, lines, fields):\n",
    "    conversations = []\n",
    "    with open(fileName, 'r', encoding='iso-8859-1') as f:\n",
    "        for line in f:\n",
    "            values = line.split(\" +++$+++ \")\n",
    "            # Extract fields\n",
    "            convObj = {}\n",
    "            for i, field in enumerate(fields):\n",
    "                convObj[field] = values[i]\n",
    "            # Convert string to list (convObj[\"utteranceIDs\"] == \"['L598485', 'L598486', ...]\")\n",
    "            lineIds = eval(convObj[\"utteranceIDs\"])\n",
    "            # Reassemble lines\n",
    "            convObj[\"lines\"] = []\n",
    "            for lineId in lineIds:\n",
    "                convObj[\"lines\"].append(lines[lineId])\n",
    "            conversations.append(convObj)\n",
    "    return conversations\n",
    "\n",
    "\n",
    "# Extracts pairs of sentences from conversations\n",
    "def extractSentencePairs(conversations):\n",
    "    qa_pairs = []\n",
    "    for conversation in conversations:\n",
    "        # Iterate over all the lines of the conversation\n",
    "        for i in range(len(conversation[\"lines\"]) - 1):  # We ignore the last line (no answer for it)\n",
    "            inputLine = conversation[\"lines\"][i][\"text\"].strip()\n",
    "            targetLine = conversation[\"lines\"][i+1][\"text\"].strip()\n",
    "            # Filter wrong samples (if one of the lists is empty)\n",
    "            if inputLine and targetLine:\n",
    "                qa_pairs.append([inputLine, targetLine])\n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’ll call these functions and create the file. We’ll call it\n",
    "*formatted_movie_lines.txt*.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to new file\n",
    "# datafile = os.path.join(corpus, \"formatted_movie_lines.txt\")\n",
    "\n",
    "# delimiter = '\\t'\n",
    "# # Unescape the delimiter\n",
    "# delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
    "\n",
    "# # Initialize lines dict, conversations list, and field ids\n",
    "# lines = {}\n",
    "# conversations = []\n",
    "# MOVIE_LINES_FIELDS = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"]\n",
    "# MOVIE_CONVERSATIONS_FIELDS = [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"]\n",
    "\n",
    "# # Load lines and process conversations\n",
    "# print(\"\\nProcessing corpus...\")\n",
    "# lines = loadLines(os.path.join(corpus, \"movie_lines.txt\"), MOVIE_LINES_FIELDS)\n",
    "# print(\"\\nLoading conversations...\")\n",
    "# conversations = loadConversations(os.path.join(corpus, \"movie_conversations.txt\"),\n",
    "#                                   lines, MOVIE_CONVERSATIONS_FIELDS)\n",
    "\n",
    "# # Write new csv file\n",
    "# print(\"\\nWriting newly formatted file...\")\n",
    "# with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
    "#     writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n",
    "#     for pair in extractSentencePairs(conversations):\n",
    "#         writer.writerow(pair)\n",
    "\n",
    "# # Print a sample of lines\n",
    "# print(\"\\nSample lines from file:\")\n",
    "# printLines(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default word tokens\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count SOS, EOS, PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 2570 sentence pairs\n",
      "Trimmed to 2570 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 3571\n",
      "\n",
      "pairs:\n",
      "['brown meat', 'drain', 'sprinkle with seasonings add salsa and warm minutes . large flour tortilla', 'sprinkle with cheddar cheese', 'oz . of meat mixture', 'sprinkle with monterey jack cheese then sprinkle with lettuce and tomato . fold in half', 'brown on warm griddle and serve with salsa and sour cream .', 'this shouldn t be a single step and may need significant explanation such as how to brown meat', 'how much seasoning to add', 'what temperature to warm it at .']\n",
      "['stuff the sausage into casings as indicated in the general sausage making directions on pages', 'tying links for to long', 'depending upon your preferance . both sizes and everything in between are considered traditional . hang the sausages in a cool', 'airy place for several hours at least', 'or until the skin is smooth', 'dry', 'and crackly . if it s too hot or humid to hang the sausages', 'refrigerate them', 'uncovered', 'for at least hours . to store', 'refrigerate for up to days', 'or freeze for longer keeping . to cook place one or more sausages in a large skillet with water to come halfway up them . bring to a makes about pounds', 'assistant needs to break this step down further . much of it doesn t make much sense .']\n",
      "['drain and rinse the beans . place in a large pot and cover by inches of fresh water . bring to a boil', 'reduce heat and simmer for hours until the beans are tender but not bursting .', '']\n",
      "['bake on a piece of parchment paper placed directly on the oven shelf for minutes or until the base is golden and crisps .', '']\n",
      "['preheat oven to f . spray a glass square baking dish with non stick cooking spray', 'or line with parchment paper .', '']\n",
      "['whip milk into beaten egg', 'using egg beater . sift flour', 'baking powder and salt together . add to egg and milk mixture a little t a time', 'alternating with maple syrup this is important . fold in melted butter . bake minutes in f oven in muffin pan .', '']\n",
      "['wipe the mushrooms and carefully remove the stalks .', 'lrg flat mushrooms grams butter medium onion peeled and chopped clv garlic peeled and washed rashers smoked pack bacon de rinded and diced grams fresh white breadcrumbs grams cheddar cheese grated tablespoon freshly chopped parsley salt and freshly ground black pepper freshly squeezed juice of lemon ml vegetable stock']\n",
      "['cut off visble bits of fat from the pork and slice them into half to get long strips . slice the shrimps horizontally into half .', '']\n",
      "['cut your green pepper and sausage into thin slices .', '']\n",
      "['remove fully cooked meatloaf from the pan and cut into slices .', '']\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 10  # Maximum sentence length to consider\n",
    "\n",
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# http://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Read query/response pairs and return a voc object\n",
    "def readVocs(datafile, corpus_name):\n",
    "    print(\"Reading lines...\")\n",
    "    # Read the file and split into lines\n",
    "    lines = open(datafile, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split(',')] for l in lines[1:]]\n",
    "    voc = Voc(corpus_name)\n",
    "    return voc, pairs\n",
    "\n",
    "import ipdb\n",
    "# Returns True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
    "def filterPair(p):\n",
    "    # Input sequences need to preserve the last word for EOS token\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "# Filter pairs using filterPair condition\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "# Using the functions defined above, return a populated voc object and pairs list\n",
    "def loadPrepareData(corpus_name, datafile, save_dir):\n",
    "    print(\"Start preparing training data ...\")\n",
    "    voc, pairs = readVocs(datafile, corpus_name)\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "#     pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    print(\"Counted words:\", voc.num_words)\n",
    "    return voc, pairs\n",
    "\n",
    "corpus_name = 'q1_recom'\n",
    "datafile = 'data/Q1-recom-elno_cleaned_data_current/trainval.csv'\n",
    "\n",
    "# Load/Assemble voc and pairs\n",
    "save_dir = os.path.join(\"data\", \"save\")\n",
    "voc, pairs = loadPrepareData(corpus_name, datafile, save_dir)\n",
    "# Print some pairs to validate\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 1496 / 3568 = 0.4193\n",
      "Trimmed from 2570 pairs to 1415, 0.5506 of total\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 3    # Minimum word count threshold for trimming\n",
    "\n",
    "def trimRareWords(voc, pairs, MIN_COUNT):\n",
    "    # Trim words used under the MIN_COUNT from the voc\n",
    "    voc.trim(MIN_COUNT)\n",
    "    # Filter out pairs with trimmed words\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # Check input sentence\n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # Check output sentence\n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "    return keep_pairs\n",
    "\n",
    "\n",
    "# Trim voc and pairs\n",
    "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Data for Models\n",
    "-----------------------\n",
    "\n",
    "Although we have put a great deal of effort into preparing and massaging our\n",
    "data into a nice vocabulary object and list of sentence pairs, our models\n",
    "will ultimately expect numerical torch tensors as inputs. One way to\n",
    "prepare the processed data for the models can be found in the `seq2seq\n",
    "translation\n",
    "tutorial <https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html>`__.\n",
    "In that tutorial, we use a batch size of 1, meaning that all we have to\n",
    "do is convert the words in our sentence pairs to their corresponding\n",
    "indexes from the vocabulary and feed this to the models.\n",
    "\n",
    "However, if you’re interested in speeding up training and/or would like\n",
    "to leverage GPU parallelization capabilities, you will need to train\n",
    "with mini-batches.\n",
    "\n",
    "Using mini-batches also means that we must be mindful of the variation\n",
    "of sentence length in our batches. To accomodate sentences of different\n",
    "sizes in the same batch, we will make our batched input tensor of shape\n",
    "*(max_length, batch_size)*, where sentences shorter than the\n",
    "*max_length* are zero padded after an *EOS_token*.\n",
    "\n",
    "If we simply convert our English sentences to tensors by converting\n",
    "words to their indexes(\\ ``indexesFromSentence``) and zero-pad, our\n",
    "tensor would have shape *(batch_size, max_length)* and indexing the\n",
    "first dimension would return a full sequence across all time-steps.\n",
    "However, we need to be able to index our batch along time, and across\n",
    "all sequences in the batch. Therefore, we transpose our input batch\n",
    "shape to *(max_length, batch_size)*, so that indexing across the first\n",
    "dimension returns a time step across all sentences in the batch. We\n",
    "handle this transpose implicitly in the ``zeroPadding`` function.\n",
    "\n",
    ".. figure:: /_static/img/chatbot/seq2seq_batches.png\n",
    "   :align: center\n",
    "   :alt: batches\n",
    "\n",
    "The ``inputVar`` function handles the process of converting sentences to\n",
    "tensor, ultimately creating a correctly shaped zero-padded tensor. It\n",
    "also returns a tensor of ``lengths`` for each of the sequences in the\n",
    "batch which will be passed to our decoder later.\n",
    "\n",
    "The ``outputVar`` function performs a similar function to ``inputVar``,\n",
    "but instead of returning a ``lengths`` tensor, it returns a binary mask\n",
    "tensor and a maximum target sentence length. The binary mask tensor has\n",
    "the same shape as the output target tensor, but every element that is a\n",
    "*PAD_token* is 0 and all others are 1.\n",
    "\n",
    "``batch2TrainData`` simply takes a bunch of pairs and returns the input\n",
    "and target tensors using the aforementioned functions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variable: tensor([[  34,   26,  215,  742,  323],\n",
      "        [  35,   25,   63,   18,   16],\n",
      "        [  16,   63,   23,  206,   23],\n",
      "        [ 287,  395,  373,  207,  138],\n",
      "        [  43,   18,  385,    7,    2],\n",
      "        [  50,   32,   29,  297,    0],\n",
      "        [ 646,   16, 1135,   21,    0],\n",
      "        [  51,   23,   21,    2,    0],\n",
      "        [  38,   33,    2,    0,    0],\n",
      "        [1147,   21,    0,    0,    0],\n",
      "        [1148,    2,    0,    0,    0],\n",
      "        [  11,    0,    0,    0,    0],\n",
      "        [ 353,    0,    0,    0,    0],\n",
      "        [ 959,    0,    0,    0,    0],\n",
      "        [ 299,    0,    0,    0,    0],\n",
      "        [ 275,    0,    0,    0,    0],\n",
      "        [  21,    0,    0,    0,    0],\n",
      "        [   2,    0,    0,    0,    0]])\n",
      "lengths: tensor([18, 11,  9,  8,  5])\n",
      "target_variable: tensor([[ 55,  55,  55,  55, 675],\n",
      "        [  2,   2,   2,   2,   7],\n",
      "        [  0,   0,   0,   0, 179],\n",
      "        [  0,   0,   0,   0,  11],\n",
      "        [  0,   0,   0,   0,   7],\n",
      "        [  0,   0,   0,   0, 134],\n",
      "        [  0,   0,   0,   0,  21],\n",
      "        [  0,   0,   0,   0,   2]])\n",
      "mask: tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 1],\n",
      "        [0, 0, 0, 0, 1]], dtype=torch.uint8)\n",
      "max_target_len: 8\n"
     ]
    }
   ],
   "source": [
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "# Returns padded input sequence tensor and lengths\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "# Returns padded target sequence tensor, padding mask, and max target length\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.ByteTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "# Returns all items for a given batch of pairs\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "\n",
    "# Example for validation\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print(\"mask:\", mask)\n",
    "print(\"max_target_len:\", max_target_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Models\n",
    "-------------\n",
    "\n",
    "Seq2Seq Model\n",
    "~~~~~~~~~~~~~\n",
    "\n",
    "The brains of our chatbot is a sequence-to-sequence (seq2seq) model. The\n",
    "goal of a seq2seq model is to take a variable-length sequence as an\n",
    "input, and return a variable-length sequence as an output using a\n",
    "fixed-sized model.\n",
    "\n",
    "`Sutskever et al. <https://arxiv.org/abs/1409.3215>`__ discovered that\n",
    "by using two separate recurrent neural nets together, we can accomplish\n",
    "this task. One RNN acts as an **encoder**, which encodes a variable\n",
    "length input sequence to a fixed-length context vector. In theory, this\n",
    "context vector (the final hidden layer of the RNN) will contain semantic\n",
    "information about the query sentence that is input to the bot. The\n",
    "second RNN is a **decoder**, which takes an input word and the context\n",
    "vector, and returns a guess for the next word in the sequence and a\n",
    "hidden state to use in the next iteration.\n",
    "\n",
    ".. figure:: /_static/img/chatbot/seq2seq_ts.png\n",
    "   :align: center\n",
    "   :alt: model\n",
    "\n",
    "Image source:\n",
    "https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_intro/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder\n",
    "~~~~~~~\n",
    "\n",
    "The encoder RNN iterates through the input sentence one token\n",
    "(e.g. word) at a time, at each time step outputting an “output” vector\n",
    "and a “hidden state” vector. The hidden state vector is then passed to\n",
    "the next time step, while the output vector is recorded. The encoder\n",
    "transforms the context it saw at each point in the sequence into a set\n",
    "of points in a high-dimensional space, which the decoder will use to\n",
    "generate a meaningful output for the given task.\n",
    "\n",
    "At the heart of our encoder is a multi-layered Gated Recurrent Unit,\n",
    "invented by `Cho et al. <https://arxiv.org/pdf/1406.1078v3.pdf>`__ in\n",
    "2014. We will use a bidirectional variant of the GRU, meaning that there\n",
    "are essentially two independent RNNs: one that is fed the input sequence\n",
    "in normal sequential order, and one that is fed the input sequence in\n",
    "reverse order. The outputs of each network are summed at each time step.\n",
    "Using a bidirectional GRU will give us the advantage of encoding both\n",
    "past and future context.\n",
    "\n",
    "Bidirectional RNN:\n",
    "\n",
    ".. figure:: /_static/img/chatbot/RNN-bidirectional.png\n",
    "   :width: 70%\n",
    "   :align: center\n",
    "   :alt: rnn_bidir\n",
    "\n",
    "Image source: http://colah.github.io/posts/2015-09-NN-Types-FP/\n",
    "\n",
    "Note that an ``embedding`` layer is used to encode our word indices in\n",
    "an arbitrarily sized feature space. For our models, this layer will map\n",
    "each word to a feature space of size *hidden_size*. When trained, these\n",
    "values should encode semantic similarity between similar meaning words.\n",
    "\n",
    "Finally, if passing a padded batch of sequences to an RNN module, we\n",
    "must pack and unpack padding around the RNN pass using\n",
    "``torch.nn.utils.rnn.pack_padded_sequence`` and\n",
    "``torch.nn.utils.rnn.pad_packed_sequence`` respectively.\n",
    "\n",
    "**Computation Graph:**\n",
    "\n",
    "   1) Convert word indexes to embeddings.\n",
    "   2) Pack padded batch of sequences for RNN module.\n",
    "   3) Forward pass through GRU.\n",
    "   4) Unpack padding.\n",
    "   5) Sum bidirectional GRU outputs.\n",
    "   6) Return output and final hidden state.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "-  ``input_seq``: batch of input sentences; shape=\\ *(max_length,\n",
    "   batch_size)*\n",
    "-  ``input_lengths``: list of sentence lengths corresponding to each\n",
    "   sentence in the batch; shape=\\ *(batch_size)*\n",
    "-  ``hidden``: hidden state; shape=\\ *(n_layers x num_directions,\n",
    "   batch_size, hidden_size)*\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "-  ``outputs``: output features from the last hidden layer of the GRU\n",
    "   (sum of bidirectional outputs); shape=\\ *(max_length, batch_size,\n",
    "   hidden_size)*\n",
    "-  ``hidden``: updated hidden state from GRU; shape=\\ *(n_layers x\n",
    "   num_directions, batch_size, hidden_size)*\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        #   because our input size is a word embedding with number of features == hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        # Convert word indexes to embeddings\n",
    "        embedded = self.embedding(input_seq)\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # Unpack padding\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        # Return output and final hidden state\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder\n",
    "~~~~~~~\n",
    "\n",
    "The decoder RNN generates the response sentence in a token-by-token\n",
    "fashion. It uses the encoder’s context vectors, and internal hidden\n",
    "states to generate the next word in the sequence. It continues\n",
    "generating words until it outputs an *EOS_token*, representing the end\n",
    "of the sentence. A common problem with a vanilla seq2seq decoder is that\n",
    "if we rely soley on the context vector to encode the entire input\n",
    "sequence’s meaning, it is likely that we will have information loss.\n",
    "This is especially the case when dealing with long input sequences,\n",
    "greatly limiting the capability of our decoder.\n",
    "\n",
    "To combat this, `Bahdanau et al. <https://arxiv.org/abs/1409.0473>`__\n",
    "created an “attention mechanism” that allows the decoder to pay\n",
    "attention to certain parts of the input sequence, rather than using the\n",
    "entire fixed context at every step.\n",
    "\n",
    "At a high level, attention is calculated using the decoder’s current\n",
    "hidden state and the encoder’s outputs. The output attention weights\n",
    "have the same shape as the input sequence, allowing us to multiply them\n",
    "by the encoder outputs, giving us a weighted sum which indicates the\n",
    "parts of encoder output to pay attention to. `Sean\n",
    "Robertson’s <https://github.com/spro>`__ figure describes this very\n",
    "well:\n",
    "\n",
    ".. figure:: /_static/img/chatbot/attn2.png\n",
    "   :align: center\n",
    "   :alt: attn2\n",
    "\n",
    "`Luong et al. <https://arxiv.org/abs/1508.04025>`__ improved upon\n",
    "Bahdanau et al.’s groundwork by creating “Global attention”. The key\n",
    "difference is that with “Global attention”, we consider all of the\n",
    "encoder’s hidden states, as opposed to Bahdanau et al.’s “Local\n",
    "attention”, which only considers the encoder’s hidden state from the\n",
    "current time step. Another difference is that with “Global attention”,\n",
    "we calculate attention weights, or energies, using the hidden state of\n",
    "the decoder from the current time step only. Bahdanau et al.’s attention\n",
    "calculation requires knowledge of the decoder’s state from the previous\n",
    "time step. Also, Luong et al. provides various methods to calculate the\n",
    "attention energies between the encoder output and decoder output which\n",
    "are called “score functions”:\n",
    "\n",
    ".. figure:: /_static/img/chatbot/scores.png\n",
    "   :width: 60%\n",
    "   :align: center\n",
    "   :alt: scores\n",
    "\n",
    "where $h_t$ = current target decoder state and $\\bar{h}_s$ =\n",
    "all encoder states.\n",
    "\n",
    "Overall, the Global attention mechanism can be summarized by the\n",
    "following figure. Note that we will implement the “Attention Layer” as a\n",
    "separate ``nn.Module`` called ``Attn``. The output of this module is a\n",
    "softmax normalized weights tensor of shape *(batch_size, 1,\n",
    "max_length)*.\n",
    "\n",
    ".. figure:: /_static/img/chatbot/global_attn.png\n",
    "   :align: center\n",
    "   :width: 60%\n",
    "   :alt: global_attn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luong attention layer\n",
    "class Attn(torch.nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # Calculate the attention weights (energies) based on the given method\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        attn_energies = attn_energies.t()\n",
    "\n",
    "        # Return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our attention submodule, we can implement the\n",
    "actual decoder model. For the decoder, we will manually feed our batch\n",
    "one time step at a time. This means that our embedded word tensor and\n",
    "GRU output will both have shape *(1, batch_size, hidden_size)*.\n",
    "\n",
    "**Computation Graph:**\n",
    "\n",
    "   1) Get embedding of current input word.\n",
    "   2) Forward through unidirectional GRU.\n",
    "   3) Calculate attention weights from the current GRU output from (2).\n",
    "   4) Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector.\n",
    "   5) Concatenate weighted context vector and GRU output using Luong eq. 5.\n",
    "   6) Predict next word using Luong eq. 6 (without softmax).\n",
    "   7) Return output and final hidden state.\n",
    "\n",
    "**Inputs:**\n",
    "\n",
    "-  ``input_step``: one time step (one word) of input sequence batch;\n",
    "   shape=\\ *(1, batch_size)*\n",
    "-  ``last_hidden``: final hidden layer of GRU; shape=\\ *(n_layers x\n",
    "   num_directions, batch_size, hidden_size)*\n",
    "-  ``encoder_outputs``: encoder model’s output; shape=\\ *(max_length,\n",
    "   batch_size, hidden_size)*\n",
    "\n",
    "**Outputs:**\n",
    "\n",
    "-  ``output``: softmax normalized tensor giving probabilities of each\n",
    "   word being the correct next word in the decoded sequence;\n",
    "   shape=\\ *(batch_size, voc.num_words)*\n",
    "-  ``hidden``: final hidden state of GRU; shape=\\ *(n_layers x\n",
    "   num_directions, batch_size, hidden_size)*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step (word) at a time\n",
    "        # Get embedding of current input word\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        # Forward through unidirectional GRU\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # Calculate attention weights from the current GRU output\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # Predict next word using Luong eq. 6\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        # Return output and final hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Training Procedure\n",
    "-------------------------\n",
    "\n",
    "Masked loss\n",
    "~~~~~~~~~~~\n",
    "\n",
    "Since we are dealing with batches of padded sequences, we cannot simply\n",
    "consider all elements of the tensor when calculating loss. We define\n",
    "``maskNLLLoss`` to calculate our loss based on our decoder’s output\n",
    "tensor, the target tensor, and a binary mask tensor describing the\n",
    "padding of the target tensor. This loss function calculates the average\n",
    "negative log likelihood of the elements that correspond to a *1* in the\n",
    "mask tensor.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single training iteration\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "The ``train`` function contains the algorithm for a single training\n",
    "iteration (a single batch of inputs).\n",
    "\n",
    "We will use a couple of clever tricks to aid in convergence:\n",
    "\n",
    "-  The first trick is using **teacher forcing**. This means that at some\n",
    "   probability, set by ``teacher_forcing_ratio``, we use the current\n",
    "   target word as the decoder’s next input rather than using the\n",
    "   decoder’s current guess. This technique acts as training wheels for\n",
    "   the decoder, aiding in more efficient training. However, teacher\n",
    "   forcing can lead to model instability during inference, as the\n",
    "   decoder may not have a sufficient chance to truly craft its own\n",
    "   output sequences during training. Thus, we must be mindful of how we\n",
    "   are setting the ``teacher_forcing_ratio``, and not be fooled by fast\n",
    "   convergence.\n",
    "\n",
    "-  The second trick that we implement is **gradient clipping**. This is\n",
    "   a commonly used technique for countering the “exploding gradient”\n",
    "   problem. In essence, by clipping or thresholding gradients to a\n",
    "   maximum value, we prevent the gradients from growing exponentially\n",
    "   and either overflow (NaN), or overshoot steep cliffs in the cost\n",
    "   function.\n",
    "\n",
    ".. figure:: /_static/img/chatbot/grad_clip.png\n",
    "   :align: center\n",
    "   :width: 60%\n",
    "   :alt: grad_clip\n",
    "\n",
    "Image source: Goodfellow et al. *Deep Learning*. 2016. http://www.deeplearningbook.org/\n",
    "\n",
    "**Sequence of Operations:**\n",
    "\n",
    "   1) Forward pass entire input batch through encoder.\n",
    "   2) Initialize decoder inputs as SOS_token, and hidden state as the encoder's final hidden state.\n",
    "   3) Forward input batch sequence through decoder one time step at a time.\n",
    "   4) If teacher forcing: set next decoder input as the current target; else: set next decoder input as current decoder output.\n",
    "   5) Calculate and accumulate loss.\n",
    "   6) Perform backpropagation.\n",
    "   7) Clip gradients.\n",
    "   8) Update encoder and decoder model parameters.\n",
    "\n",
    "\n",
    ".. Note ::\n",
    "\n",
    "  PyTorch’s RNN modules (``RNN``, ``LSTM``, ``GRU``) can be used like any\n",
    "  other non-recurrent layers by simply passing them the entire input\n",
    "  sequence (or batch of sequences). We use the ``GRU`` layer like this in\n",
    "  the ``encoder``. The reality is that under the hood, there is an\n",
    "  iterative process looping over each time step calculating hidden states.\n",
    "  Alternatively, you ran run these modules one time-step at a time. In\n",
    "  this case, we manually loop over the sequences during the training\n",
    "  process like we must do for the ``decoder`` model. As long as you\n",
    "  maintain the correct conceptual model of these modules, implementing\n",
    "  sequential models can be very straightforward.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "    # Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # Perform backpropatation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training iterations\n",
    "~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "It is finally time to tie the full training procedure together with the\n",
    "data. The ``trainIters`` function is responsible for running\n",
    "``n_iterations`` of training given the passed models, optimizers, data,\n",
    "etc. This function is quite self explanatory, as we have done the heavy\n",
    "lifting with the ``train`` function.\n",
    "\n",
    "One thing to note is that when we save our model, we save a tarball\n",
    "containing the encoder and decoder state_dicts (parameters), the\n",
    "optimizers’ state_dicts, the loss, the iteration, etc. Saving the model\n",
    "in this way will give us the ultimate flexibility with the checkpoint.\n",
    "After loading a checkpoint, we will be able to use the model parameters\n",
    "to run inference, or we can continue training right where we left off.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
    "\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "\n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        # Extract fields from batch\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (iteration % save_every == 0):\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Evaluation\n",
    "-----------------\n",
    "\n",
    "After training a model, we want to be able to talk to the bot ourselves.\n",
    "First, we must define how we want the model to decode the encoded input.\n",
    "\n",
    "Greedy decoding\n",
    "~~~~~~~~~~~~~~~\n",
    "\n",
    "Greedy decoding is the decoding method that we use during training when\n",
    "we are **NOT** using teacher forcing. In other words, for each time\n",
    "step, we simply choose the word from ``decoder_output`` with the highest\n",
    "softmax value. This decoding method is optimal on a single time-step\n",
    "level.\n",
    "\n",
    "To facilite the greedy decoding operation, we define a\n",
    "``GreedySearchDecoder`` class. When run, an object of this class takes\n",
    "an input sequence (``input_seq``) of shape *(input_seq length, 1)*, a\n",
    "scalar input length (``input_length``) tensor, and a ``max_length`` to\n",
    "bound the response sentence length. The input sentence is evaluated\n",
    "using the following computational graph:\n",
    "\n",
    "**Computation Graph:**\n",
    "\n",
    "   1) Forward input through encoder model.\n",
    "   2) Prepare encoder's final hidden layer to be first hidden input to the decoder.\n",
    "   3) Initialize decoder's first input as SOS_token.\n",
    "   4) Initialize tensors to append decoded words to.\n",
    "   5) Iteratively decode one word token at a time:\n",
    "       a) Forward pass through decoder.\n",
    "       b) Obtain most likely word token and its softmax score.\n",
    "       c) Record token and score.\n",
    "       d) Prepare current token to be next decoder input.\n",
    "   6) Return collections of word tokens and scores.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        # Iteratively decode one word token at a time\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            # Obtain most likely word token and its softmax score\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            # Record token and score\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            # Prepare current token to be next decoder input (add a dimension)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        # Return collections of word tokens and scores\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate my text\n",
    "~~~~~~~~~~~~~~~~\n",
    "\n",
    "Now that we have our decoding method defined, we can write functions for\n",
    "evaluating a string input sentence. The ``evaluate`` function manages\n",
    "the low-level process of handling the input sentence. We first format\n",
    "the sentence as an input batch of word indexes with *batch_size==1*. We\n",
    "do this by converting the words of the sentence to their corresponding\n",
    "indexes, and transposing the dimensions to prepare the tensor for our\n",
    "models. We also create a ``lengths`` tensor which contains the length of\n",
    "our input sentence. In this case, ``lengths`` is scalar because we are\n",
    "only evaluating one sentence at a time (batch_size==1). Next, we obtain\n",
    "the decoded response sentence tensor using our ``GreedySearchDecoder``\n",
    "object (``searcher``). Finally, we convert the response’s indexes to\n",
    "words and return the list of decoded words.\n",
    "\n",
    "``evaluateInput`` acts as the user interface for our chatbot. When\n",
    "called, an input text field will spawn in which we can enter our query\n",
    "sentence. After typing our input sentence and pressing *Enter*, our text\n",
    "is normalized in the same way as our training data, and is ultimately\n",
    "fed to the ``evaluate`` function to obtain a decoded output sentence. We\n",
    "loop this process, so we can keep chatting with our bot until we enter\n",
    "either “q” or “quit”.\n",
    "\n",
    "Finally, if a sentence is entered that contains a word that is not in\n",
    "the vocabulary, we handle this gracefully by printing an error message\n",
    "and prompting the user to enter another sentence.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    ### Format input sentence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # Use appropriate device\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # Decode sentence with searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    # indexes -> words\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input('> ')\n",
    "            # Check if it is quit case\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            # Normalize sentence\n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            # Evaluate sentence\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            # Format and print response sentence\n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Bot:', ' '.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Model\n",
    "---------\n",
    "\n",
    "Finally, it is time to run our model!\n",
    "\n",
    "Regardless of whether we want to train or test the chatbot model, we\n",
    "must initialize the individual encoder and decoder models. In the\n",
    "following block, we set our desired configurations, choose to start from\n",
    "scratch or set a checkpoint to load from, and build and initialize the\n",
    "models. Feel free to play with different model configurations to\n",
    "optimize performance.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "# Configure models\n",
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "#attn_model = 'general'\n",
    "#attn_model = 'concat'\n",
    "hidden_size = 500\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "loadFilename = None\n",
    "checkpoint_iter = 4000\n",
    "#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
    "#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
    "#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "\n",
    "# Load model if a loadFilename is provided\n",
    "if loadFilename:\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint['en']\n",
    "    decoder_sd = checkpoint['de']\n",
    "    encoder_optimizer_sd = checkpoint['en_opt']\n",
    "    decoder_optimizer_sd = checkpoint['de_opt']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    voc.__dict__ = checkpoint['voc_dict']\n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Training\n",
    "~~~~~~~~~~~~\n",
    "\n",
    "Run the following block if you want to train the model.\n",
    "\n",
    "First we set training parameters, then we initialize our optimizers, and\n",
    "finally we call the ``trainIters`` function to run our training\n",
    "iterations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateFile(encoder, decoder, searcher, voc, filename, targetname):\n",
    "    text = list(csv.reader(open(filename, 'rt')))\n",
    "    target = list(csv.reader(open(targetname, 'rt')))\n",
    "    responses = []\n",
    "#     input_sentence = ''\n",
    "    for input_sentence in text:\n",
    "        try:\n",
    "            # Get input sentence\n",
    "#             input_sentence = input('> ')\n",
    "            # Check if it is quit case\n",
    "#             if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            # Normalize sentence\n",
    "            input_sentence = normalizeString(input_sentence[0])\n",
    "            # Evaluate sentence\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            # Format and print response sentence\n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            responses.append(' '.join(output_words))\n",
    "#             print('Bot:', ' '.join(output_words))\n",
    "        except KeyError:\n",
    "            responses.append(' ')\n",
    "#             print(\"Error: Encountered unknown word.\")\n",
    "    return text, target, responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers ...\n",
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n",
      "Iteration: 1; Percent complete: 0.0%; Average loss: 0.0538\n",
      "Iteration: 2; Percent complete: 0.1%; Average loss: 0.0979\n",
      "Iteration: 3; Percent complete: 0.1%; Average loss: 0.0481\n",
      "Iteration: 4; Percent complete: 0.1%; Average loss: 0.1000\n",
      "Iteration: 5; Percent complete: 0.1%; Average loss: 0.0365\n",
      "Iteration: 6; Percent complete: 0.1%; Average loss: 0.0507\n",
      "Iteration: 7; Percent complete: 0.2%; Average loss: 0.0443\n",
      "Iteration: 8; Percent complete: 0.2%; Average loss: 0.0531\n",
      "Iteration: 9; Percent complete: 0.2%; Average loss: 0.0358\n",
      "Iteration: 10; Percent complete: 0.2%; Average loss: 0.1084\n",
      "Iteration: 11; Percent complete: 0.3%; Average loss: 0.0670\n",
      "Iteration: 12; Percent complete: 0.3%; Average loss: 0.0407\n",
      "Iteration: 13; Percent complete: 0.3%; Average loss: 0.0676\n",
      "Iteration: 14; Percent complete: 0.4%; Average loss: 0.0595\n",
      "Iteration: 15; Percent complete: 0.4%; Average loss: 0.0581\n",
      "Iteration: 16; Percent complete: 0.4%; Average loss: 0.0494\n",
      "Iteration: 17; Percent complete: 0.4%; Average loss: 0.0672\n",
      "Iteration: 18; Percent complete: 0.4%; Average loss: 0.0275\n",
      "Iteration: 19; Percent complete: 0.5%; Average loss: 0.0493\n",
      "Iteration: 20; Percent complete: 0.5%; Average loss: 0.0349\n",
      "Iteration: 21; Percent complete: 0.5%; Average loss: 0.0344\n",
      "Iteration: 22; Percent complete: 0.5%; Average loss: 0.0490\n",
      "Iteration: 23; Percent complete: 0.6%; Average loss: 0.0486\n",
      "Iteration: 24; Percent complete: 0.6%; Average loss: 0.0470\n",
      "Iteration: 25; Percent complete: 0.6%; Average loss: 0.0488\n",
      "Iteration: 26; Percent complete: 0.7%; Average loss: 0.0577\n",
      "Iteration: 27; Percent complete: 0.7%; Average loss: 0.0625\n",
      "Iteration: 28; Percent complete: 0.7%; Average loss: 0.0339\n",
      "Iteration: 29; Percent complete: 0.7%; Average loss: 0.0640\n",
      "Iteration: 30; Percent complete: 0.8%; Average loss: 0.0441\n",
      "Iteration: 31; Percent complete: 0.8%; Average loss: 0.0368\n",
      "Iteration: 32; Percent complete: 0.8%; Average loss: 0.0512\n"
     ]
    }
   ],
   "source": [
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 4000\n",
    "print_every = 1\n",
    "save_every = 500\n",
    "\n",
    "# Ensure dropout layers are in train mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# Run training iterations\n",
    "print(\"Starting Training!\")\n",
    "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Evaluation\n",
    "~~~~~~~~~~~~~~\n",
    "\n",
    "To chat with your model, run the following block.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">  \n",
      "Bot:  to greased oven to break\n",
      ">  \n",
      "Bot:  to greased oven to break\n",
      ">  \n",
      "Bot:  to greased oven to break\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    466\u001b[0m         \"\"\"\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-3de5b829fbba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Begin chatting (uncomment and run the following line to begin)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mevaluateInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-697ba0b24fe3>\u001b[0m in \u001b[0;36mevaluateInput\u001b[0;34m(encoder, decoder, searcher, voc)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# Get input sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0minput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'> '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;31m# Check if it is quit case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'q'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    851\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m         )\n\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 883\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    884\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluateInput(encoder, decoder, searcher, voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "----------\n",
    "\n",
    "That’s all for this one, folks. Congratulations, you now know the\n",
    "fundamentals to building a generative chatbot model! If you’re\n",
    "interested, you can try tailoring the chatbot’s behavior by tweaking the\n",
    "model and training parameters and customizing the data that you train\n",
    "the model on.\n",
    "\n",
    "Check out the other tutorials for more cool deep learning applications\n",
    "in PyTorch!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/Q1-recom-elno_cleaned_data_current/test_text.csv'\n",
    "targetname = 'data/Q1-recom-elno_cleaned_data_current/test_target.csv'\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "text, target, responses = evaluateFile(encoder, decoder, searcher, voc, filename=filename, targetname=targetname)\n",
    "\n",
    "with open('data/Q1-recom-elno_cleaned_data_current/test_predict.csv', 'wt') as f:\n",
    "    for l in responses:\n",
    "        f.write('{}\\n'.format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BLEU = 0.00, 3.4/0.9/0.3/0.0 (BP=0.000, ratio=0.103, hyp_len=932, ref_len=9031)\\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "res = os.popen('perl multi_bleu.perl data/Q1-recom-elno_cleaned_data_current/test_target.csv < data/Q1-recom-elno_cleaned_data_current/test_predict.csv')\n",
    "res.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ',\n",
       " ' ',\n",
       " ' to greased oven to break up the',\n",
       " ' to greased pot to break up the',\n",
       " ' ',\n",
       " ' a wooden spoon to coat the flour',\n",
       " ' ',\n",
       " ' a fine paste . to coat the',\n",
       " ' ',\n",
       " ' to coat pan to coat the',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to greased oven to break up the',\n",
       " ' to greased pot to use .',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' a little oil on the pot .',\n",
       " ' to greased oven to coat the turkey',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to coat pan to coat the beans',\n",
       " ' a few more minutes and set aside .',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to coat pan to coat the pan',\n",
       " ' to greased pot to break up the',\n",
       " ' ',\n",
       " ' ',\n",
       " 'how to do the cheese . to',\n",
       " ' to coat the  to coat the',\n",
       " ' to coat the rice well . to',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to greased pot to break up the',\n",
       " ' a little bit tablespoon water to coat',\n",
       " ' a little chicken . to coat',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to greased oven to break up the',\n",
       " ' a wooden spoon to coat the until',\n",
       " ' to coat pan to coat the until',\n",
       " ' a little chicken stock water and serve',\n",
       " ' to greased oven to this slightly',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to coat the rice with the fork .',\n",
       " ' to coat the beans with the beans .',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' a fine paste . to coat the',\n",
       " ' ',\n",
       " ' to greased melted to top . to',\n",
       " ' to greased pot to break .',\n",
       " 'add flour and roll again . to dry spoon',\n",
       " ' to greased oven to break up the',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to pot . to coat the beans',\n",
       " ' ',\n",
       " ' ',\n",
       " 'show how step is done . to coat the',\n",
       " 'stir the rice to coat the until egg are',\n",
       " 'how to make sure the cream cheese and chopped parsley',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' a fine paste . to dry spoon',\n",
       " ' a link to this step to',\n",
       " ' to greased pot to break up the',\n",
       " ' to greased melted to top . to',\n",
       " ' ',\n",
       " ' a little bit . to greased oven',\n",
       " ' to coat the rice well . season with',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to coat pan to coat the turkey',\n",
       " ' to  to break up the turkey',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to greased pot to use .',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to greased oven to flatten chicken',\n",
       " ' a link to this step to prevent',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'to show a demo of how to do',\n",
       " ' a little  a few mixed',\n",
       " ' to greased oven to break up the',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to greased oven to break up the',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to coat pan to coat the until',\n",
       " ' a link to break step up .',\n",
       " ' to greased oven to break',\n",
       " ' ',\n",
       " ' ',\n",
       " 'amount of salt and pepper to use . to',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to greased pot to break .',\n",
       " ' to greased oven to turn',\n",
       " ' ',\n",
       " ' to steam the pan to use',\n",
       " ' to greased melted to break up the',\n",
       " ' to  to  to ',\n",
       " ' a few times just to combine .',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'add all the tart ingredients and set aside .',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to coat the turkey with the seasoning .',\n",
       " ' to  to  to ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' a wooden spoon to coat the cup',\n",
       " ' ',\n",
       " ' ',\n",
       " ' a few fat to use butter',\n",
       " ' a little chicken . to steam',\n",
       " ' ',\n",
       " 'soy sauce and peanut butter with a large bowl',\n",
       " ' ',\n",
       " ' to coat the  to coat the',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to greased pot to break .',\n",
       " 'then add the vanilla extract . to coat the',\n",
       " ' to coat the tomatoes . to',\n",
       " ' ',\n",
       " ' a little chicken . to coat',\n",
       " 'in a bowl to coat the until well .',\n",
       " 'drain it and rinse with cold water . to',\n",
       " ' ',\n",
       " 'show video of how to do it .',\n",
       " ' to greased pot to break .',\n",
       " 'cup stick unsalted butter to coat the syrup butter',\n",
       " ' ',\n",
       " ' to greased oven to coat the bake',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to greased pot to break .',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to coat the rice well . to',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " 'what it looks like when sirupy to coat the',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' a little  a few mixed',\n",
       " ' to greased pan to break up',\n",
       " 'whisk together the oil and use the mixture',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' a few more minutes and set aside .',\n",
       " ' a little chicken stock water and serve',\n",
       " ' ',\n",
       " 'drain off and what size spoon . to coat',\n",
       " ' to coat pan to coat the turkey',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to greased oven to break up the',\n",
       " ' a few more minutes and set aside .',\n",
       " ' a few more minutes and stir well',\n",
       " ' to greased pot to break .',\n",
       " ' ',\n",
       " ' to greased oven to prevent burning',\n",
       " 'remove the skin from the tomatoes and roll them',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' a little chicken stock to coat the',\n",
       " ' to coat pan to coat the until',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to greased oven to break up .',\n",
       " ' to greased pot to break up serve',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to  to break up the broccoli',\n",
       " ' to  to turn high',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to greased pot to break .',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to greased boil to break up the',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' a link to this step to soften',\n",
       " ' to greased oven to coat the turkey',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to greased oven to degrees .',\n",
       " ' to greased pot to break .',\n",
       " ' ',\n",
       " ' to greased pot to use .',\n",
       " ' ',\n",
       " 'sugar and vanilla extract until smooth . to',\n",
       " ' to coat the rice until very smooth',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to greased pot to break .',\n",
       " ' to steam the pan .',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to  to break up the processor',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to greased pan to use .',\n",
       " ' ',\n",
       " ' ',\n",
       " ' a little chicken . to coat',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to greased oven to break up the',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to greased oven to use .',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to coat the turkey with the seasoning .',\n",
       " ' ',\n",
       " ' to coat the  to coat the',\n",
       " ' ',\n",
       " 'then add the sliced onions and saute for about minutes',\n",
       " ' ',\n",
       " ' a wooden spoon . add the garlic',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' a little chicken . to greased',\n",
       " ' to greased oven to keep make degrees',\n",
       " ' to greased boil to drain',\n",
       " ' ',\n",
       " ' ',\n",
       " 'then the confectioners sugar a fine paste . garnish',\n",
       " ' ',\n",
       " ' ',\n",
       " ' a wooden spoon to coat the until',\n",
       " ' to coat the syrup to coat the',\n",
       " ' to greased oven to break up',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to coat the until well . to',\n",
       " ' to  to coat the processor should',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to greased oven to break up .',\n",
       " ' a wooden spoon to coat the rice',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' to coat the turkey with the seasoning .',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' ',\n",
       " ' a wooden spoon to coat the rice',\n",
       " ' ',\n",
       " ' to greased oven to flatten oven',\n",
       " 'about minutes . to soften to coat the']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['cups packed light brown sugar tablespoons margarine tablespoons vegetable shortening cups dark molasses tablespoon baking soda cup boiling water cups all purpose flour sifted tablespoon ground cloves tablespoons ground ginger tablespoon ground cinnamon'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['image of finished product'],\n",
       " [],\n",
       " [],\n",
       " ['peeled and cut into inch thick rounds bechamel sauce cup butter cups hot milk tablespoons flour eggs cup grated kefalograviera cheese or parm teaspoon salt'],\n",
       " [],\n",
       " ['visualized instruction'],\n",
       " [],\n",
       " ['how the chicken will look like after frying .'],\n",
       " [],\n",
       " ['tell exact amounts of ingredients'],\n",
       " ['place the cookie crusts in the freezer and make the banana ice cream'],\n",
       " ['show a video of this step and show the correct consistency .'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['visualized instructions for clarifying purposes'],\n",
       " ['make the pink coconut cream .'],\n",
       " [],\n",
       " [],\n",
       " ['baking dish'],\n",
       " ['show images or video of this step . suggest turning the oven on or have the assistant turn the oven on to the correct temperature .'],\n",
       " ['whl brie stk butter melted phyllo dough'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['ingredients ounces canned chickpeas rinsed and drained ounces canned coconut milk full fat cup onion diced cups sweet red pepper diced ounces handfuls of fresh spinach tbsp avocado oil for cooking curry spice blend your choice cup water tsp salt i used tbsp medium curry powder tsp garam marsala tsp turmeric tsp red chili flakes'],\n",
       " ['how slices cut'],\n",
       " [],\n",
       " ['cook onions'],\n",
       " [],\n",
       " ['show the best way to mash the bananas .'],\n",
       " ['laptop'],\n",
       " ['find out how to bake fish bun'],\n",
       " ['about minutes .'],\n",
       " [],\n",
       " ['show designs and patterns to make it look nice'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['baking sheet with parchment paper'],\n",
       " [],\n",
       " ['cilantro'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['show a picture on what the marks should look like'],\n",
       " [],\n",
       " [],\n",
       " ['visualized instructions'],\n",
       " [],\n",
       " ['crushed tbsp cream tbsp butter tbsp crushed dried chilli tbsp white vineger'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['suggest alternative ingredient .'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['show video of how to fit dough into dish'],\n",
       " [],\n",
       " ['olives and oregano . simmer on low for minutes .'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['show a video of this step'],\n",
       " ['ingredients ounces canned chickpeas rinsed and drained ounces canned coconut milk full fat cup onion diced cups sweet red pepper diced ounces handfuls of fresh spinach tbsp avocado oil for cooking curry spice blend your choice cup water tsp salt i used tbsp medium curry powder tsp garam marsala tsp turmeric tsp red chili flakes'],\n",
       " [],\n",
       " [],\n",
       " ['suggest an oil substitute .'],\n",
       " ['lrg flat mushrooms grams butter medium onion peeled and chopped clv garlic peeled and washed rashers smoked pack bacon de rinded and diced grams fresh white breadcrumbs grams cheddar cheese grated tablespoon freshly chopped parsley salt and freshly ground black pepper freshly squeezed juice of lemon ml vegetable stock'],\n",
       " ['specify estimated cooling time after cakes are cooked'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['serser'],\n",
       " ['show the recommended dough texture desired by the end of this step'],\n",
       " [],\n",
       " [],\n",
       " ['it should recommend alternate ingredients if the user asks for them .'],\n",
       " ['simmer the white wheat with . litres of water over low heat for at least minutes add pandan leave minutes later till wheat soften . stir in sugar to taste and serve with coconut cream milk .'],\n",
       " [],\n",
       " ['image or video of process'],\n",
       " [],\n",
       " ['crushed teaspoons salt teaspoon dill weed teaspoon coarsely ground pepper lbs . pork and beef cubes mushrooms optional cherry tomatoes optional small cooked potatoes'],\n",
       " [],\n",
       " ['whl brie stk butter melted phyllo dough'],\n",
       " [],\n",
       " [],\n",
       " ['clarify what this means'],\n",
       " [],\n",
       " ['ingredients ounces canned chickpeas rinsed and drained ounces canned coconut milk full fat cup onion diced cups sweet red pepper diced ounces handfuls of fresh spinach tbsp avocado oil for cooking curry spice blend your choice cup water tsp salt i used tbsp medium curry powder tsp garam marsala tsp turmeric tsp red chili flakes'],\n",
       " ['how much water to put in each time and how long to wait in between before adding more'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['we should know what the temperature of the oven should be .'],\n",
       " ['showing picture of cooking'],\n",
       " ['recommend the size of the mixing bowl .'],\n",
       " [],\n",
       " ['visualized instructions for clarifying purposes'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['the best type of veggies to use with the dip .'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['tell the user how much cheese is required to put inside'],\n",
       " ['add a timer for minutes'],\n",
       " [],\n",
       " ['cilantro'],\n",
       " [],\n",
       " [],\n",
       " ['suggest alternative ingredients'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['show a video of this step .'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['improved visualization'],\n",
       " ['add the mixture and stir until combined'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['cilantro'],\n",
       " ['you will need pounds whole pork shoulder tablespoon black pepper tablespoons chili powder tablespoon garlic powder tablespoon paprika teaspoon ground cumin ounces good ale or dark beer cloves garlic chopped'],\n",
       " [],\n",
       " [],\n",
       " ['visualized instructions'],\n",
       " ['a picture of the squash on the baking sheet .'],\n",
       " [],\n",
       " ['tips for preventing burned quinoa may be useful here .'],\n",
       " [],\n",
       " [],\n",
       " ['remove the pot and simmer it on low heat again for another minutes together with pandan leaves'],\n",
       " [],\n",
       " ['show video of how to check the temp of the oil .'],\n",
       " ['provide a timer for the hr in fridge'],\n",
       " [],\n",
       " ['provide a link to this step'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['it should show you how to cut the strawberry fans that you will garnish the cheesecake with .'],\n",
       " [],\n",
       " ['to taste'],\n",
       " [],\n",
       " [],\n",
       " ['freshly ground teaspoon paprika teaspoon celery seed teaspoon hot sauce teaspoon fresh lemon juice tablespoons brown sugar'],\n",
       " [],\n",
       " [],\n",
       " ['you will need pounds whole pork shoulder tablespoon black pepper tablespoons chili powder tablespoon garlic powder tablespoon paprika teaspoon ground cumin ounces good ale or dark beer cloves garlic chopped'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['chop walnuts and a few leaves of herbs'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['optimize the ingredients needed see where to shop for needed ingredients'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['a link to this step'],\n",
       " ['cups oz g blended fresh raspberries'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['show a video of this step'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['the garlic and onions . cook until softened . stir in the tbsp of taco seasoning just to combine .'],\n",
       " ['peeled and cut into inch thick rounds bechamel sauce cup butter cups hot milk tablespoons flour eggs cup grated kefalograviera cheese or parm teaspoon salt'],\n",
       " [],\n",
       " ['visualized instructions'],\n",
       " ['whl brie stk butter melted phyllo dough'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['how you prepare one .'],\n",
       " [],\n",
       " ['a timer should be provided to keep track of the hrs'],\n",
       " ['remove pan from heat and stir in baking soda'],\n",
       " [],\n",
       " [],\n",
       " ['tell the user how much of the top to cut off .'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['once helping out with all the ingredients with the item introduction if needed'],\n",
       " ['so a video would be nice for the user and he won t feel lost duringg this step'],\n",
       " [],\n",
       " [],\n",
       " ['tbps shrimp paste pound ground pork teaspoon hot madras curry powder sprigs of purple basil regular green basil is fine if purple basil stalks of fresh lemongrass each about inches long'],\n",
       " [],\n",
       " ['divided water pound sweet vinegar slaw wholes wheat buns'],\n",
       " [],\n",
       " ['show an instructional video to how to cook the vegetables .'],\n",
       " [],\n",
       " [],\n",
       " ['how to cook perfect bacon .'],\n",
       " [],\n",
       " ['preheat oven to degrees .'],\n",
       " [],\n",
       " [],\n",
       " ['improved visualization'],\n",
       " ['thawed or fresh blueberries quart balsamic vinegar cup sugar inch lime peel cut strips from lime cinnamon stick long'],\n",
       " [],\n",
       " ['softened'],\n",
       " ['how it should appear when properly combined'],\n",
       " ['explain what this means .'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['peeled and sliced into coins large mushrooms caps removed and sliced a large yellow onion cans of white beans drained tsp sage tsp thyme lots of fresh ground black pepper white pepper a shake or two pinch or two of salt olive oil'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['exact number of garlic cloves'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['and reference size of said meatballs .'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['set timer for minutes'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['peeled and sliced into coins large mushrooms caps removed and sliced a large yellow onion cans of white beans drained tsp sage tsp thyme lots of fresh ground black pepper white pepper a shake or two pinch or two of salt olive oil'],\n",
       " [],\n",
       " ['image of the ingredients ?'],\n",
       " ['list each ingredient to mix and how to mix it properly'],\n",
       " ['more specific on time to cook rice .'],\n",
       " [],\n",
       " ['how to tell if your sausage is browned .'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['and possibly recommend the best method .'],\n",
       " [],\n",
       " [],\n",
       " ['a link to this step'],\n",
       " [],\n",
       " ['improved visualization'],\n",
       " [],\n",
       " [],\n",
       " ['info about what consistency to expect'],\n",
       " [],\n",
       " ['show how to cut up the meats .'],\n",
       " [],\n",
       " ['provide different toppings .'],\n",
       " [],\n",
       " [],\n",
       " ['show in a video'],\n",
       " ['kilogram pernil de de cordeiro azeite sal e pimenta berinjelas cebolas roxas cortadas em oregano seco alecrim fresco dentes de alho salsinha grams latas de de tomate pelado vinagre de vinho tinto pimenta chili seca anchovas'],\n",
       " ['plus cup olive oil cup dry vermouth cups crushed tomatoes'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['show how it is done'],\n",
       " ['specific video instructions on how to do this'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['display how to properly mix ingredients'],\n",
       " [],\n",
       " ['etc . fill the mold and freeze for six hours or overnight until firm . you will need a fresh trout . it is very important to use a fresh fish . on a cutting board cut the head off using a sashimi slicer . gut the fish and wash it in cold running water . use your free hand to keep the fish from slipping while you fillet it . keep the blade horizontal and cut just above the skeleton of the fish from head to tail . turn the fish over and cut the second fillet . remove the rib bones from the fillets . remove any remaining bones with tweezers . remove the skin . in a small bowl gently blend vinegar salt and pepper into sour cream set aside . reserve egg to be cut into wedges for garnish chop other egg . in a large bowl combine egg potato meat cottage cheese celery and onion . gently blend in sour cream mixture . cover and chill . garnish with egg wedges . show video of this entire process . mix all ingredients in a small bowl . if not using immediately store in an airtight container or ziplock bag . when crepes are easily handled and malleable remove from refrigerator along with filling . allow it to cool completely pour some of your strawberry and cheeses mixture into it and drizzle with your strawberry syrup juice . place all of the banana ice cream ingredients in a blender and wait for to unthaw a bit . now blend the ingredients until you reach that creamy ice cream consistency . place all of the banana ice cream ingredients in a blender and wait for to unthaw a bit . now blend the ingredients until you reach that creamy ice cream consistency . for the filling cream the butter and shortening slowly add the sugars while beating . add the evaporated milk vanilla and lemon extract . mix on medium speed until completely smooth and fluffy . when the cakes are done and cooled use a tooth pick to make three small holes in the bottom of each one . move the toothpick around the inside of each cake to create space for the filling . using a cake decorator or plastic bag or pastry bag inject each cake with filling through all three holes . mix dry ingredients and add yeast place water onion skins oil salt and pepper in a quart pan . one dozen eggs cup olive oil teaspoon salt teaspoon pepper outer dried browned skins from onions water to cover eggs cover the dough with plastic wrap and allow to rest till double in bulk about hour . after the dough has risen pour out onto a clean flat surface . punch down the dough to release some of the air . devide the dough into pieces and shape each into a round form . place the rounds onto a parchment lined cookie sheet . cover with plastic warp and allow to rise for another minutes . brush the rolls with the remaining eggwash and bake into a preheated f oven for minutes . remove and cool on a wire rack before served . prepare the lemon filling continue cooking for about one minute then remove and transfer to a plate . bake for minutes in the oven at c turning them over half way through bake for minutes in the oven at c turning them over half way through heat a skillet with tablespoons of extra virgin olive oil and saute zucchini until slightly crumbled and golden . add salt and pepper to taste . remove and repeat for other quesadilla . combine chocolate milk and cinnamon in a sauce pan . stir over low heat until smooth and shiny reserve about cup for later . in a large bowl combine wafer crumbs marshmallow and sugar . stir in chocolate mixture . the muffins are great to eat the same day but even better and more moist the day after . i always eat some the first day and more the next day . . . .i mean who can resist freshly baked muffins ? recommend how the muffins should be stored . refrigerator covered ? add white beans and cabbage stirring to combine . saute onion in butter until soft . stir in curry and flour and heat an additional mins . slowly add the milk stirring constantly as sauce thickens . add chopped eggs and heat thru . pour over muffins garnish with parsley or cilantro and serve . curry powder provide video on how to do this step . melt the chocolate over double boiler . add tablespoon of butter and mix until dissolved . all the charm and elegance of the last century with all the comfort and amenities of the next . comments make the poppy seed pastries in advance and store them in the freezer . at serving time just defrost them they will stay crisp and everyone will think they were freshly baked . make extra pastries to follow the purim tradition of sharing shalach manos and give a basket of these delicious confections as gifts to family and friends . show suitable baskets for sale . assembling divide the dough into four pieces and run through the machine adding flour for the icing place the icing sugar in a large bowl . in a pot combine sugar and cornstarch . add in cherry lemon juice and stir until smooth . cook over medium heat stirring until the mixture boils and thickens . remove from heat and let cool . drizzle the chocolate cheesecake with cherry sauce or simply just dust with icing sugar . malzemeleri yogurun . can show video on how to make this food . add bacon to skillet and fry until crisp remove with a slotted spoon and reserve . chill the ramekins in the fridge until the custard is thoroughly set . place tablespoons raisins in each tart . divide syrup mixture among tarts . bake until filling browns and puffs and top appears dry about minutes . transfer racks cool . remove tarts from pans . serve with frozen yogurt . can show how the tarts look like after baking . pour batter into an oiled pyrex pan and put in the oven for minutes or until a knife inserted in the cake comes out clean . show how a pan is properly oiled . show how the knife should look . have the assistant automatically set the timer for the oven or a reminder to check on the cake . use the leftover ranch sauce . make sure the cheese and vegetables are covered or wet with the custard mixture . heat tablespoon of the oil and gently fry the onions and celery until soft . stir in tomatoes and thyme and cook for about minutes to soften tomatoes . place in base of shallow dish . in a small bowl add oatmeal yogurt milk and combine . let it sit for few minutes or several hours in the refrigerator . then add apples banana dried fruits and stir until well combined . top with nuts before serving . in a hot pan add the tvp . cook for minutes then add the mixed spices over top . make sure you incorporate the spices very well into the tvp . let cook for minutes until the liquid has evaporated . explain what tvp is . lazy way to serve let cool slightly then toss in parsley season to taste and serve . deglaze with some chicken broth then add a can of coconut milk some more chicken broth fish sauce cup lime juice cilantro prepare a vegetable stock and keep it aside . the process of making the stock simmer sauce stirring occasionally until slightly thickened . about five minutes . simmer sauce and stir occasionally add the wet ingredients to the dry and mix well . batter will be fairly thick i added a dash of buttermilk at this point because my batter was dry . stir in cool and pulse in a food processor until fine . stir fry for a minute . add cups water and bring to a boil . brush the cake all over with the warmed apricot jam . this recipe yields bagels . . clean and gut the fish remove backbone rinse and drain . remove the seeds and cut the lemons into small pieces . microwave on high for minute or until dip is warm . heating dip allows for the flavors to mesh together . adj seasonings . heat oil in a pan add fennel seeds chopped onion and cook until slightly translucent . www .cremedelacrumb .blogspot .com bring to boil and cook at medium heat for minutes . place vinegar honey salt and roasted garlic in a food processor . puree until garlic is chopped very fine . provide an approximate time that it will take to puree the garlic . this recipe yields servings . make it four servings bring . litres of water to boil in a saucepan used my small endo thermal magic cooker then add in the drain white wheat and let it simmer for about minutes . bring . litres of water to boil in a saucepan used my small endo thermal magic cooker then add in the drain white wheat and let it simmer for about minutes . the bartlett pear is the best to use for liquor making . it is juicy sweet and smooth . avoid one with cuts bruises dark spots or decay . add more water in cup increments if needed . return marshmallow mixture to refrigerator . strain the broth and return it to the stock pot to heat . pour cups gin in a glass . set aside let cool for minutes in a cake pan and then transfer to a cooling rack to cool completely . dissolve the sugar with half a cup of water in a pan over a medium heat . this is one sorbet where i have never found it necessary to use fresh blackcurrants . in fact the frozen ones are better and available all year round . the same applies to raspberries . carefully transfer the baking sheet to the oven . brush the edges with an egg wash the egg white from one egg and place another pasta piece over the top . seal the edges with your fingers a fork or whatever puts a smile on your face . a link to this step in a heated skillet saute sesame oil hijiki carrots age and tofu all completely drained of water . when ingredients have acquired a shiny appearance add dashijiru shoyu sake and mirin . reduce heat to low and cook stirring frequently until liquid has been absorbed . turn off heat and add green beans . combine ingredients in a non reactive bowl . cups finely shredded purple cabbage red pepper seeded and finely diced cup mayonnaise cup whipping cream cup cider or rice wine vinegar teaspoon salt teaspoon black pepper freshly ground teaspoon paprika teaspoon celery seed teaspoon hot sauce teaspoon fresh lemon juice tablespoons brown sugar place dressing in a covered container and refrigerate for several hours stirring occasionally so that the sugar dissolves and the flavors blend . add ching s secret schezwan chutney and toss well . allow to cool then cut or decorate as you desire . enjoy ! remove from the oven and toss with coarse salt . cool . serve with rice and green salad . sprinkle toasted almonds over top of the tvp dish and rice . show how to toast almonds . then add boiled thoor dhaal how to boil thoor dhaal place in the center of the oven and bake for about minutes or until the top is golden brown . remove from the oven and cool completely . garnish with sprigs of fresh mint . blend shortening egg and milk . mix flour baking powder salt and sugar . when skillet is fully heated lift skillet from burner spray with cooking spray then pour cup batter onto the middle of the skillet then turn with your wrist quickly to cover entire bottom of skillet with crepe mixture . preheat oven to f . line a baking sheet with parchment paper or silpat baking mat . cup sugar serve slightly warm or at room temperature garnished with small mounds of whipped cream and chocolate shavings . the quiche should cool down for minutes before it can be sliced . cook for about hour until dark browned and even blackening in places . remove from oven . lower the oven to degrees . you will need pounds whole pork shoulder tablespoon black pepper tablespoons chili powder tablespoon garlic powder tablespoon paprika teaspoon ground cumin ounces good ale or dark beer cloves garlic chopped bake for about minutes . for the tomato onion relish remove skillet from heat and promptly lift fillet from hot liquid using a karamel rengi aldiginda kremayi ekleyin eriyinceye kadar karistirip ocaktan alin ve sogumaya birakin . soguduktan sonra yukarda hazirlanan kremaya ilave edip karistirin . place in serving dish and sprinkle with sauteed sesame seeds . . now add coriander leaves and pour hot water . tomato tsp jeera powder tsp garam masala green chillies bay leaf salt as per taste cup hot water gms mutton chops tsp ginger garlic paste tsp coriander powder whole cardamom cloves cinnamon fresh coriander leaves oil bake for minutes then remove the wax paper with weights and bake for minutes . bake for mins then remove the wax paper add drained tuna stir all together . allow to cook for a few minutes . add drain tuna and stir place in a shallow airtight container drizzle juice over kebabs and sprinkle with mint . cover and chill . in butter lightly brown rice and vermicelli from rice a roni . stir in hot water chicken raisins rice a roni flavor packet and curry powder . cover and simmer minutes . garnish top with peanuts and coconut . servings . clarification drain any fat from meat and top with mushrooms and shredded cheese . turn oven to broil and cook until cheese is melted . brown ground beef drain fat . place in large casserole dish . cover with cream of chicken soup diluted with can water . salt and pepper . place frozen tater tots over top and bake uncovered at degrees for minutes . add cheese and continue to bake minutes more . in a large skillet heat olive oil on medium high . add your cherries and walnuts . add slices of bacon on top . drain peppercorns and rinse under warm water . heat extra butter in small pan add peppercorns and cook minute . remove chicken breasts from baking dish arrange on serving platter . keep warm . reduce pan dripping to tablespoon by simmering over gentle heat for to minutes . add pan dripping to peppercorns along with combined egg yolks cream sour cream and mustard . stir over low heat until sauce saute onions mushrooms sesame seed and garlic in butter until browned . add lemon grass ginger chillies peppercorns and chicken broth bring to boil and cook on moderate heat until reduced by about one half . double wrap each panettone in plastic and store in the refrigerator . bring to room temperature before serving . using a thin pancake turner turn crepe . cook tagliatelle in a large pot of boiling salted water for mins . grams tagliatelle italian flat noodle or spaghetti grams grated parmesan cheese freshly ground black pepper pesto sauce cup fresh sweet basil leaves cloves garlic peeled tablespoons pine nuts lightly toast pine nuts for about mins teaspoon salt milliliters extra virgin olive oil ms water note i used tsp wonderslim to replace the vegetable oil . what on earth is wonderslim ? i use canola oil accessible everywhere . if wondrslim oil is similar to canola oil additional recipes can be found at www .peanutbureau .ca . add spinach and cook for minute ingredients ounces canned chickpeas rinsed and drained ounces canned coconut milk full fat cup onion diced cups sweet red pepper diced ounces handfuls of fresh spinach tbsp avocado oil for cooking curry spice blend your choice cup water tsp salt i used tbsp medium curry powder tsp garam marsala tsp turmeric tsp red chili flakes dried apples pears and prunes are cooked in water until they are soft . carefully combine the flour with the wet mix . display how to properly mix and what it should look like in a large bowl combine the sugar ginger nutmeg cinnamon salt and baking soda . mix well . cup sugar cover and chill dough about hour or until easy to handle . i did mine over night but thats not necessary . heat a tablespoon of butter in a non stick pan then toast sandwich on both sides until golden brown . in a salad bowl combine the apples walnuts celery and raisins . roast it on a medium flame for five minutes . mix together thoroughly the flour baking powder cheddar cheese and spinach . tastes like champagne . bots white grape juice lrg bottle club soda lrgs bottle up heat a medium skillet to medium with one tablespoon of olive oil . serve with chips or veggies preferably cold . if you can t wait eat it right away ! drizzle teaspoon olive oil over top of garlic wrap tightly in foil and place in oven on separate baking sheet for minutes . add curry powder and paprika and stir to coat the chicken or mushrooms . add the curry powder and paprika to give it the tast with an electric mixer cream butter both sugars . add eggs one at a time . add applesauce and pumpkin and then flour mixture beating until just combined . fill cupcake liners about three quarters full . bake approximately minutes but check frequently after minutes . cool completely . to show a demo of how to do add half of the white chocolate into the bottom of the tin foil lined pan . spread evenly with an offset spatula . let harden . to speed up the process place in the fridge for about minutes or until it has set up . show a video of this step and show the correct consistency . cut chicken on the diagonal into thick strips about two inches wide . how the chicken will look after cutting after that mix the bean sprouts and cucumber in a bowl and season well before filling the bean curd tofu pouches . or beef with the meat magic . place meats in a heavy pot and let sit while you dice up the onion . add the onion spices soup base . add enough water to cover . cook on medium heat until meat is easily removed from bones . strain reserving juice . show video of the following step . heat the oil in a casserole and brown the meat all over in batches until it has a good colour . remove and set aside pre heat wok or ordinary pan with coocking oil or olive oil mix all ingredients together and store in an air tight container . cut the onion in slices . to serve scoop into serving dishes or into the individual dishes . serve hot with noodles or rice . pour batter into baking pan and set inside a larger baking pan . add enough hot water to the large baking pan to reach half way up the sides of the smaller baking pan . pour the batter into baking pan blend the molasses eggs and vanilla into the butter mixture until it becomes a uniform color . might be good to show what color yield servings . whisk together powdered sugar orange zest and orange juice until smooth fry in olive oil they should talk about the duration of frying or if said food needs to be flipped . cut rounds with a cookie cutter . put approx . tsp jam in the center and fold to form a semi circle . advance prep peel and cube the potatoes and cook until fork tender . mash . set aside . mediums onions finely chopped cup unsalted butter cups cooked and mashed potatoes see note cup creme fraiche salt pepper set the sauce aside in a bowl and place in the fridge to chill . wash the rice well . in a pan put the rice water and salt . bring it to a boil . lower the heat . cover and cook till the rice is soft and the water is absorbed . stir in the coconut milk . simmer till the coconut milk is absorbed and the rice is very soft and creamy . cool slightly and turn out into a flat dish . level the sides and top with a knife . when cold cut kiribath into diamond shapes . can be served as the main course with the accompaniments suggested above . soak pipis in water for hours to remove sand . sprinkle the chopped coriander leaves and mix well remove from the fire and keep it aside . bake for minutes squeeze lemon and set aside . put sugar in a glass . add ice water stirring until sugar is dissolved . add lemon juice and pour over crushed ice . decorations heat remaining oil and quickly fry the tuna steaks to brown on each side . place tuna steaks on top of vegetables season then pour over white wine . cover . bake at degrees for to minutes until fish is tender . top off with ice approx of the pitcher to make the crust combine flour sugar and salt in a large bowl . add butter and combine until mixture resembles coarse meal . add ice water and mix until mixture just begins to hold together . for the dough beat the butter with the salt and sugar until light . add the flavorings and beat until smooth . add eggs then continue beating until the mixture is emulsified and smooth and looks like buttercream . if the mixture remains curdled warm the bottom of the mixing bowl in a pan of warm water for a second or two and continue beating rewarm the bowl as necessary until the mixture is smooth . add one third of the flour and mix in then another eggs . repeat with another third of the flour the last eggs and the last third of the flour . beat ground beef may be used instead of ground chicken . drizzle some oil on the grill and the fish in a separate bowl whisk together the egg ricotta and oil . remove keep warm . in same pan melt tablespoons butter over medium heat . saute shallots shortly then add the mushrooms and garlic . remove keep warm in same pan melt tablespoons butter over medium heat saute shallots shortly then add the mushrooms and garlic same procedure for veg . manchurian with gravy or dry but instead of using only cauliflower use finely chopped minced vegetables and bind with some cornflour or bread crumbs and make small lumps the size of a pingpong ball . place fruit cut side down on the grill and cook for minutes or until warmed through . large grapefruit ruby red or standard tablespoons canola oil tablespoon brown sugar reduce heat and whisk in cheese stirring constantly until melted and fully incorporated . show what this looks like . transfer the pasta to a large bowl mix in the mushrooms and coat with the herb dressing . preheat the oven to degrees . add citrus juice and cover tightly with foil . cover the pan and cook for hour on very low heat . cover the pan and cook for hour on very low heat . set up a work surface with a bowl of water a pastry brush and a flat area covered in wax or parchment paper to rest your tortellinis . video on the steps to do this . put a little olive oil in a pan . add red onion and cook minute . add crushed garlic i always have on hand the kind in a jar cooked sliced chicken breasts and some dried oregano . heat to minutes then add to sliced roma tomatoes and heat until chicken starts to darken . tips on how to slice an onion thinly or without crying once your onions broccoli and chicken are cooked and your cheeses grated please grate your own there are very few instances when pre grated is okay ! huge huge flavor and texture difference for the price . it s time to make your sauce . this sauce is essentially a flavored roux . a roux is a cooked mixture of fat and flour and many sauces and gravies begin with one . master a roux and you are halfway to making a delicious sauce . colored sugar can be used for decoration . pancakes drop batter by cups onto a hot greased griddle . grill flank steak over medium heat for minutes per side until desired doneness minutes dependent on how thick the steak is will be medium to medium rare . measurements of thickness of steak to now how much to vary cooking time by to achieve the right cooking of the steak . also maybe mention the hand trick for showing how firm the different levels of steak cookedness are . topping to enclose and steam tamales proceed according to directions in basic tamales . reduce heat and continue stirring until pudding coats has thickened to desired consistency it should easily coat the back of a wooden spoon . place into the oven and bake for minutes or until done . preheat oven to thread cantaloupe honeydew and watermelon chunks alternately onto each of inch skewers . place the balls onto a baking try lined with grease proof paper and flatten them out into circles . leave approx cm of space between each cookie . pour strained custard over the top of the bread . using a spatula gently press down on bread slices so they soak up custard . place dish in a large roasting pan add enough warm water to pan to come halfway up the sides of the baking dish . using your hands or a large spoon mix bowl contents until uniformly blended . tbps shrimp paste pound ground pork teaspoon hot madras curry powder sprigs of purple basil regular green basil is fine if purple basil stalks of fresh lemongrass each about inches long tbsp . hemp hearts in a non stick frying pan heat the vegetable oil over medium heat and sear for minutes each side until golden brown . now add maida suji baking powder to the milk powder and mix well . then add khoya butter ghee egg and saffron threads and mix well to make a thick batter . make small balls from this batter malli in urdu . and keep aside . this recipe yields large haystacks . glaze divide the circle of puff pastry and lay the crescents cocottine obtained in trying to form a basket leaving out of the container part of the sheet to form the crunchy bits . divide the circle of puff pantry this is how you roll . enjoy ! toss the dressing with the apple nut mixture . serve the salad over lettuce leaves . set garlic aside to cool . add wine beef broth bay leaf garlic and thyme . simmer covered for minutes stirring frequently . transfer to plastic container and place in the freezer for hours before serving . add in the zucchini and mushrooms mix in another tablespoon of olive oil and mix well with the potatoes and sweet potatoes . roast for another minutes . make sure the veggies have nice caramelization . what caramelization looks like . combine the rest of the ingredients in a separate bowl and add chicken . preheat the oven to f . cut the squash in half lengthwise and scoop out the seeds and stringy center . oil each half gently with olive oil or cooking spray put face down on a cookie sheet and bake until tender about minutes . a timer let remind you evr mins plunge the tomatoes into boiling water for seconds . remove with a slotted spoon transfer to a bowl of iced water . remove from the water slip off their skins . chop the tomatoes set aside . in a large pot saute the mustard seeds in the oil over a medium heat until they begin to sputter pop . add the garlic bay leaf turmeric rice stirring until the rice becomes translucent minutes . add the tomatoes reduce the heat to a simmer for minutes . add the broth bring back to a boil remove from the heat add the cilantro allow it to steam covered for minutes . remove the bay leaf fluff with a fork stirring in the cilantro . a way to stop the oil spitting everywhere . note onion are numbers medium size stir in the beans parsley lemon juice the remaining butter teaspoon salt and teaspoon pepper . cook until heated through to minutes . serve with the grains cook until mixture begins to thicken approximately minutes . stir in remaining herbs spices chicken and half of the cheese . show proper thickness of mixture heat oil in medium skillet . add onion and garlic and saute on medium high heat until onion is transparent . add remaining ingredients and simmer for min . remove from heat . in quart saucepan saute onion in butter until tender . stir in flour mustard salt and pepper until blended . stir in cups milk and salmon liquid mixture and worcestershire sauce . cook stirring constantly until milk mixture boils and thickens . step recipe is about what i remember the spices being that we added to the cooked red beans hence this coconut condiment added to some nice red beans and served over rice should be excellent with the addition of some added chiles o course . when oil begins to smoke add scallops . be careful ! i used long grilling tongs these babies splatter and that oil is h o t hot ! this recipe yields servings . quickly mix together using a large spoon spatula or your hands first approach the shrimp . i bought the frozen variety but feel free to buy fresh if you d like . thaw the shrimp peel and de vein . i like to remove the entire tail but you may choose to keep it . cook on high heat until it boils then lower the heat and simmer for at least minutes or longer until the beef is tender . cook in a saucepan until just starting to boil and set aside to cool . next chop up the chiles garlic and onion in the processor . then add the vinegar cumin lime juice tequila and the cooled tomatillo chayote mixture . hit the high speed button on the food processor and let it go until the whole mess is sauce . it s really green a little tart and hotter than hades ! pour ml pints of the cream into a saucepan and add the vanilla pods and orange rinds . bring to the boil and once boiling reduce the heat and simmer until reduced by . take out the rinds and pods and scrape the inside of the pods into the cream . assistant should go through each part individually . even though this is listed as a single step it s rather long and complicated and needs to be broken up . add butter parsley and season with pepper put together the meat grinder attachment and sausage stuffer piece and insert it into the head of the mixer . tighten well . show a video of this step . in a soup pot heat glugs of olive oil over med low heat add c c o g until translucent about minutes . tips for making a perfect recipe . prepare crust . sprinkle gelatin over water to soften . mix sugar cornstarch and salt in top of double boiler . gradually stir in eggnog . clarify what a double boiler is . say how to prepare the crust . the original recipe notes that the base can be thickened by adding a few boiled potatoes . add a few boiled potatoes remove the pot from stove and place it into the magic cooker cover and let the wheat continue to cook through using the heat from the pot . remove the pot from stove and place it into the magic cooker cover and let the wheat continue to cook through using the heat from the pot . add in the baking powder flour and ground cinnamon . stir until everything is combined . bring just to a slow boil stirring occasionally then remove from heat . chill the cake for another hours . stir fry onions and garlic until cooks place in the oven and bake for about minutes until the duck is golden and the juices run clear . butter the shiny side of a piece of aluminum foil and fit it butter side down tightly against the frozen crust . put the tart pan on a baking sheet and bake for minutes . preheat the oven to f . skin the pheasant and remove all the meat . chop into fairly small pieces and place in a bowl . how to skin pheasant directions fry bacon until crispy in a large non stick frying pan . set bacon aside to drain . add onions to bacon grease except one tablespoon . add diced potatoes mix with the onions . add chives parsley garlic and salt and pepper stirring well between each addition . when thoroughly mixed pour potatoes in a bowl lined with a paper towel . scramble eggs in same pan . when eggs are almost set add potato mixture and crumbled bacon mix thoroughly . all seasonings are optional . experiment with the recipe to find the combination for you . eggs may be served separately with the potato bacon mi reduce oven temperature to degrees f degrees c . in a mixing bowl whisk together wine chicken broth and remaining tablespoons of oil pour over hens . continue roasting about minutes longer or until hens are golden brown and juices run clear . baste with pan juices every minutes . place pie crust in a deep dish glass pie plate and crimp edges . achaari murg is ready store in an airtight container . peel and cube your potatoes then place in a large pot drain strawberries reserve syrup . dissolve jello and add reserved juice . chill to partially set beat until light and fluffy . stir in whipped cream and chill until spreadable . transfer cups of mixture and add strawberries . split cake into layers and frost with mixture . chill until serving time . wash and pat dry chicken . place chicken breast side up in a low roasting pan . rub chicken with fresh bay leaves and then rub piri piri sauce all over the chicken . place the pork in the middle rack of the oven . these are usually served as a pickle or relish . green peppers cup white wine vinegar tablespoons water cup olive oil bay leaf teaspoon salt teaspoon sugar peppercorns bruised cloves garlic peeled place dough onto a lightly floured surface and quickly and gently knead . combine mango onion garlic and chiles into the avocado mixture . when onions are slightly browned add butter berbere tomato paste and remaining spices . stir well add cup water stir . don t add too much water at this point because once the chicken has been added the amount of liquid will increase . if time allows let simmer to minutes . cook together until the shrimp just begin to pink . this can happen quickly depending on the size of your shrimp . rub chicken breasts with salt and pepper . beat in melted butter . continue to add broth occasionally and stir until rice is cooked the way you would with any risotto . time limit for the rice to cook . don t assume someone knows time limit of risotto . on the work surface open the boned lamb out to make as close to a rectangle as you can carefully trim off as much of the fat as possible and dispose of it . show video of this process . cook spinach according to the directions on package and drain . drain the artichokes and chop them . mix the artichokes and spinach with the remaining ingredients . put in greased quart dish and bake at degrees for minutes . serve with tortilla chips . also good served with sour cream and salsa on the side . the assistant should recommended more instructions in cooking . once beet water has cooled to room temperature add the vinegar honey salt pepper cinnamon and cloves . stir well . in a large bowl mix all the vegetables and the thyme sprigs together . add the olive oil and honey . salt and pepper . roast in oven at degrees for about minutes or until vegetables are tender stirring occasionally . prepare the veggie queso dip ahead of time . suggesting video serve warm with creme fraiche which balances really well with the sweetness of the tart or a good vanilla ice cream . drop from tip of a teaspoon onto well oiled cookie sheets to make inch rounds or spoon into well oiled inch molds . if syrup hardens while shaping drops heat slowly until just melted . serve with wedges of lime . coat fish thoroughly with seasonings . mix sour cream sugar and vanilla . spoon on top of cooled cheesecake and bake for another minutes . sprinkle with cinnamon optional . how much sugar needed for cream note this recipe is excellent for freezing . audio is only required . drain in paper towel . repeat with the rest of the dough . serve with crostini bread crackers or veggie sticks . this dip is to be served luke warm to room temperature . so if it sits out for a while that s ok . but if you d prefer you can serve it warm hot . best type of veggies to dip it in . disregard the directions on the box of cake mix instead beat the egg whites until stiff . combine them with the cake mix and water and beat until thoroughly blended about minutes . pour the batter into the molds fillng each one about inch . bake for about minutes or until the cake is golden brown and a toothpick stuck in the center comes out clean . combine all ingredients into a blender on high . combine dry ingredients in a mixing bowl . spray both sides of fish thoroughly with vegetable spray . redcurrant granita mango granita coffee granita mix together all ingredients and knead . place in large greased bowl let rise until double then punch down and knead slightly again . place in large bread pans greased and rise again . bake at degrees for hour . brush tops of loaves with butter when removed from oven . how long to knead to make the cream place butter and shortening in a mixing bowl and at low speed gradually beat in the sugar and vanilla . turn the mixer on high and beat for to minutes until filling is light and fluffy . if needs clarification a link to be provided so that if we ask clarify then we have a option to follow the link and learn . variation spoon dessert topping over prepared pudding fresh fruit or pound cake . after minutes increase the oven temp . to f . and continue to bake for an additional minutes . in a large dutch oven saute bacon for minutes over medium high heat . with a slotted spoon remove bacon and set aside . boil the taro for an hour or until it is what it looks like over cooked soak gelatin in cup water for minutes heat it up in a microwave for minute . let it sit for minutes the gelatin water will become completely clear . pour into pie shell . puree fruit with powdered sugar and xocai activ . mix in egg yolk and cream . blend well . chill . churn in ice cream maker . serves a demonstration video heat oil in a wok and deep fry the samosa till it becomes light brown on both the sides . suggest exact time limit . for a large loaf pan bake for minutes . cups flour teaspoons baking powder teaspoon soda teaspoon salt cup sugar cup shortning eggs beaten cup bananas mashed cup chopped walnuts optional transfer peppers to plastic wrap or bag . close the bag or seal with plastic wrap and let peppers steam and cool . pulse together flour sugar salt powder in bowl of food processor . now make the peanut dipping sauce . set the dough aside and cover it with plastic food wrap . let the dough sit for min . place baguette slices on baking sheet and bake until golden and crispy about minutes . pour chocolate peanut butter mixture over chow mein noodles and stir in marshmallows . gently combine . pour the batter mixture over chow mein noodles and stir in marshmallows . gently combine . in a bowl mix all of the chocolate cookie crust ingredients together until well combined . it helps if you mix the ingredients with your hands too . in a mini cupcake pan place the mini cupcake liners inside and press the chocolate crust on the bottom of the liners . in a bowl mix all of the chocolate cookie crust ingredients together until well combined . it helps if you mix the ingredients with your hands too . in a mini cupcake pan place the mini cupcake liners inside and press the chocolate crust on the bottom of the liners . cook frozen mixed berries and tablespoons sugar in a heavy medium saucepan over medium heat until mixture resembles jam and is reduced to cup stirring frequently about minutes . cool jam mixture . sprinkle the nonpareils on top of the chocolate . don t tap it again otherwise your sprinkles will sink down in . place into the fridge to chill . this will take about hour . show an image of this step . to assemble peel mushroom caps and break in pieces . add oyster crabs and wine cover and let stand one hour . melt butter add first mixture and cook eight minutes . add flour and cook two minutes . season with salt cayenne and nutmeg then add heavy cream . just before serving add egg yolks slightly beaten and brandy . show a website with this recipe . discard end slices of loaf . spread cup ml butter evenly over one side of each bread slice . arrange six of the bread slices buttered side up in a single layer on the bottom of prepared dish . cut a seventh slice in half and wedge in among other slices to fill the bottom of the dish sprinkle half of the apricots evenly over the top . repeat with remaining seven bread slices and apricots for a second layer set aside . remove from pans and cool on rack . stir and allow cooking until the liquid is dry . stir the food combine egg substitute buttermilk oatmeal molasses oil . mix cornstarch and flour into milk . mix well . add to cooking vegetables . toss in peas . show how to properly mix ingredients heat oil in big pot and fry onion and garlic for minute . then add the chicken and continue cooking for minutes . add all the spices . stir everything together for a few minutes . let cook until chicken is tender . add the rest of the ingredients . adjust seasoning and serve with steamed rice . how to cook the rice stir fry the chinese barbecued pork and the scallions for minute . add your spices provide alternative spices . in a pan add water and sugar and heat till boil . after few boils turn off flame add rose essences and keep aside this sugar syrup . line your pan with aluminum foil or use disposable pans add gruyere and parmesan cheeses stir until melted . season to taste with salt and pepper . use hot . if making up to day ahead let cool then cover and chill reheat in a microwave safe bowl in a microwave oven at full power for about minute . do not reboil after adding egg yolk . on each serving plate place a pear half at the o clock position fanning it across the plate . divide the prosciutto into portions and drape one portion across the middle of each serving plate covering the top half of the pear fan . garnish each plate with cup dressed greens placed in the o clock position above the prosciutto . show example preheat oven to degrees . makes quart . makes quart . in a bowl whisk the flour baking powder baking soda cinnamon and salt . place the steaks in a dish and add half of the cognac . let it soak each steaks on both sides for a minute or so . slice the chicken add and mix set aside for later use .']]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
