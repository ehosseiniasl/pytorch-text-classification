{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "> Some of the explanations in this tutoarial are inspired by chapters 3 and 8 in the book _\"Python Machine Learning\"_ by Sebastian Raschka\n",
    "\n",
    "First, let's introduce what Logistic Regression is. \n",
    "\n",
    "[Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression) is a classification model that is very easy to implement and performs very well on linearly separable classes. It is one of the most widely used\n",
    "algorithms for classification in industry too, which makes it attractive to play with.\n",
    "\n",
    "_Very_ simplistically explained, Logistic Regression works as follows:\n",
    "\n",
    "![Logistic Regression](./images/logistic_regression.png)\n",
    "\n",
    "First we will define the input for our algorithm. The imput will be each sample in whatever dataset we are working with. Each sample will consist of several features. For example, if we're working with housing price prediction, the features for each sample could be the size of the house, number of rooms, etc. We'll call the input vector **X**.\n",
    "\n",
    "For the algorythm to learn, we need to define variables that we can adjust accordingly to what we want to predict. We will create a vector of _weights_ (**W**) that the model will adjust in order to predict more accurately. The process of adjusting those weights is what we call **learning**.\n",
    "\n",
    "For every input sample, we will perform a dot product of the features by the weights **XW**. This product is sometimes referred as _net input_. This will give us a real number. Since in this particular problem we want to _classify_ (positive/negative), we need squash this number in the range [0, 1]. This will give us the _probability_ of a positive event. A function that does precisely that is called **sigmoid**. The sigmoid function looks like this:\n",
    "\n",
    "![sigmoid](./images/sigmoid.svg)\n",
    "\n",
    "What sigmoid is doing is basically transforming big inputs into a value close to 1, and small inputs into a value close to 0. This is exactly what we want. \n",
    "\n",
    "We will do this for every sample in our training set and compute the errors. To calculate the error we only need to compare our prediction with the true label for each sample. We will sum the square errors of all the samples to get a global prediction error. This will be our **cost function**.\n",
    "\n",
    "A cost function is then something we want to minimize. **Gradient descent** is a method for finding the minimum of a function of multiple variables, such like the one we're dealing with here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gradient descent\n",
    "\n",
    "> Watch [this](https://www.youtube.com/watch?v=IHZwWFHWa-w) video for a visual introduction to Gradient Descent\n",
    "\n",
    "Once all our training samples have been computed and the error calculated with our cost function, we need to _minimize_ that cost function. A method for doing that is gradient descent. There are many [articles](https://en.wikipedia.org/wiki/Gradient_descent) that contain detailed explanations _and_ implementations of GD, so let's not do this here. However is good to have an intuition.\n",
    "\n",
    "For illustration purposes, let's think about a function with two parameters. Something like this one:\n",
    "\n",
    "![Gradient Descent](./images/Gradient_descent.png)\n",
    "\n",
    "Gradient descent will try to find the minimum of the function. To do so, we calculate the slope of the function at a certain point, and move towards the direction that makes the function decrease. There are some things to have in mind though.\n",
    "\n",
    "As you can see a function can have one or several _local minimum_. In a local minimum, the slope will be zero and GD will \"think\" it's found the global minimum. To avoid this, we can choose a bigger \"step\" when we move towards the minimum. The \"size\" of the step towards the minimun is what we call the **learning rate**, and it's another adjustable parameter.\n",
    "\n",
    "We need to be careful here: If we choose a too small learning rate, we can get stuck in a local minimum. If we instead choose a too big learning rate, we risk overshooting the global minimum. We need to experiment, and the adecuate learning rate depends on the particular problem and the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training process\n",
    "\n",
    "In order for logistic regression to learn, we need to repeat the process descrived before several times. Each one of these times is called an **epoch**. The number of epochs to run depends on the problem and the training data. It is... yes, another tunnable parameter of the algorithm.\n",
    "\n",
    "The set of all tunnable parameters is called **hyperparameters** of the model.\n",
    "\n",
    "Like with the leatning rate, we need to be careful when choosing the number of epochs: If we train too many epochs, we risk **overfitting**. This means that our model will \"memorize\" the training data and will generalize badly when presented new data. \n",
    "\n",
    "If we train too little, it will fail to find any pattern and the prediction accuracy will be very low. This is known as **underfitting**.\n",
    "\n",
    "There are techniques that help prevent overfitting. These **regularization** techniques are out of the scope of this tutorial, but... guess! It's also something to tune and experiment with :)\n",
    "\n",
    "This is why when training a model you need to set aside a _test dataset_ in order to know the accuracy of your algorithm in unknown data. The test dataset will **never** be used during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "\n",
    "Let's first of all have a look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recipedetails</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pure soup batch blender use immers hand blende...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>use bread flour semolina flour turn dough oil ...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>note author say 1 tsp dri mint could use dri m...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>let cool complet store airtight contain keep f...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>add veget stir fri minut two</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>shrimp bask cornmeal focu grit</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>leftov fill wa done ad sauc add broth leftov f...</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>preheat oven 375f</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fri onion garlic larg pot add splash water pre...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>add yogurt</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       recipedetails label\n",
       "0  pure soup batch blender use immers hand blende...   yes\n",
       "1  use bread flour semolina flour turn dough oil ...    no\n",
       "2  note author say 1 tsp dri mint could use dri m...    no\n",
       "3  let cool complet store airtight contain keep f...   yes\n",
       "4                       add veget stir fri minut two   yes\n",
       "5                     shrimp bask cornmeal focu grit    no\n",
       "6  leftov fill wa done ad sauc add broth leftov f...    no\n",
       "7                                  preheat oven 375f    no\n",
       "8  fri onion garlic larg pot add splash water pre...   yes\n",
       "9                                         add yogurt    no"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Load the data into a DataFrame\n",
    "train = pd.read_csv('../pytorch-text-classification/data/Q1-yesno-elno_cleaned_data_current/trainval.csv', encoding='latin-1',sep=',')\n",
    "test = pd.read_csv('../pytorch-text-classification/data/Q1-yesno-elno_cleaned_data_current/test.csv', encoding='latin-1',sep=',')\n",
    "\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       pure soup batch blender use immers hand blende...\n",
       "1       use bread flour semolina flour turn dough oil ...\n",
       "2       note author say 1 tsp dri mint could use dri m...\n",
       "3       let cool complet store airtight contain keep f...\n",
       "4                            add veget stir fri minut two\n",
       "5                          shrimp bask cornmeal focu grit\n",
       "6       leftov fill wa done ad sauc add broth leftov f...\n",
       "7                                       preheat oven 375f\n",
       "8       fri onion garlic larg pot add splash water pre...\n",
       "9                                              add yogurt\n",
       "10      uncov bake sauc 375 f oven brown 45 minut one ...\n",
       "11      cook noodl boil water 3 min ceas stick togeth ...\n",
       "12      put freezer minimum 2 month mix fresh tonic fr...\n",
       "13      note ensur maximum light allow dough ferment p...\n",
       "14          fill glass one third full add water crush ice\n",
       "15      preheat oven 350 degre f greas flour 9 x5 loaf...\n",
       "16                               spiral zucchini set asid\n",
       "17      grill bell pepper ga charcoal grill grill side...\n",
       "18      heat 1 teaspoon oil larg skillet medium heat s...\n",
       "19      assembl 13x9x2 inch bake dish arrang layer pot...\n",
       "20      wash bean soak overnight cold water drain put ...\n",
       "21      make fill cut pineappl quarter crosswis 1 3 in...\n",
       "22      dish plate garnish cucumb crisp shallot spring...\n",
       "23                                     season salt pepper\n",
       "24      recip wa linda najjar seattl washington herb c...\n",
       "25      use big skillet add oil saut leek onion garlic...\n",
       "26      spoon poppi seed fill pastri bag blend sugar n...\n",
       "27                                                 serv 4\n",
       "28                     either dust ice sugar marzipan ice\n",
       "29      add rice 2 cup boil stock cover cook tender 20...\n",
       "                              ...                        \n",
       "2538    butter pipe spoon snail desir transfer pastri ...\n",
       "2539    pot soften rice less 2 part water 1 part rice ...\n",
       "2540    medium bowl stir togeth flour ginger bake soda...\n",
       "2541    serv rice green salad sprinkl toast almond top...\n",
       "2542            divid dough four piec run machin ad flour\n",
       "2543                              preheat oven 300f degre\n",
       "2544       make sure chees veget cover wet custard mixtur\n",
       "2545             preheat oven 350 degre rack posit center\n",
       "2546    spoon butternut squash pure small medium mix bowl\n",
       "2547    place dish roast dish pour hot water come half...\n",
       "2548    blend first 3 ingredi blender dilut juic add c...\n",
       "2549    ravioli cutter use use glass cut circl like us...\n",
       "2550                                      wash lemon well\n",
       "2551    return heavi bottom pan place medium heat cust...\n",
       "2552       place fruit cut side grill cook 3 5 minut warm\n",
       "2553                add chop chocol stir heat melt chocol\n",
       "2554    place bread rack let cool room temperatur befo...\n",
       "2555    use hand combin pork mapl syrup 1 4 cup chop w...\n",
       "2556                                anchor bar origin way\n",
       "2557    cut pork belli cube use broad side knife pat s...\n",
       "2558    risotto cook shallot larg pan oil minut brown ...\n",
       "2559    pot wash ha flavor cook meatbal cook onion 5 m...\n",
       "2560             4 spray side fish thoroughli veget spray\n",
       "2561    place chicken breast bowl add soy sauc teriyak...\n",
       "2562                                   spread warm muffin\n",
       "2563    skillet wok heat 1 tbsp 15 ml oil medium high ...\n",
       "2564    allow stand 24 hour allow ice harden befor dec...\n",
       "2565    place greas cooki sheet cover wet tea towel al...\n",
       "2566    heat oil wok deep fri samosa till becom light ...\n",
       "2567    preheat oven 350 degre spray 9x9 pan browni po...\n",
       "Name: recipedetails, Length: 2568, dtype: object"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.recipedetails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the structure of a twit varies a lot between twit and twit. They have different lengths, letters, numbers, extrange characters, etc. \n",
    "\n",
    "It is also important to note that **a lot** of words are not correctly spelled, for example the word _\"Juuuuuuuuuuuuuuuuussssst\"_ or the word _\"frie\"_ instear of _\"friend\"_\n",
    "\n",
    "This makes it hard to mesure how positive or negative are the words withing the corpus of twits. If they were all correct dictionary words, we could use a **lexicon** to punctuate words. However because of the nature of social media language, we cannot do that. \n",
    "\n",
    "So we need a way of scoring the words such that words that appear in positive twits have greater score that those that appear in negative twits.\n",
    "\n",
    "But first... how do we represent the twits as vectors we can input to our algorithm?\n",
    "\n",
    "### Bag of words\n",
    "\n",
    "One thing we could do to represent the twits as equal-sized vectors of numbers is the following:\n",
    "\n",
    "* Create a list (vocabulary) with all the unique words in the whole corpus of twits. \n",
    "* We construct a feature vector from each twit that contains the counts of how often each word occurs in the particular twit\n",
    "\n",
    "_Note that since the unique words in each twit represent only a small subset of all the words in the bag-of-words vocabulary, the feature vectors will mostly consist of zeros_\n",
    "\n",
    "Let's construct the bag of words. We will work with a smaller example for illustrative purposes, and at the end we will work with our real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 13,\n",
       " 'is': 7,\n",
       " 'amazing': 2,\n",
       " 'ml': 9,\n",
       " 'the': 12,\n",
       " 'best': 3,\n",
       " 'yes': 15,\n",
       " 'it': 8,\n",
       " 'am': 1,\n",
       " 'not': 10,\n",
       " 'sure': 11,\n",
       " 'about': 0,\n",
       " 'how': 6,\n",
       " 'going': 5,\n",
       " 'to': 14,\n",
       " 'end': 4}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "twits = [\n",
    "    'This is amazing!',\n",
    "    'ML is the best, yes it is',\n",
    "    'I am not sure about how this is going to end...'\n",
    "]\n",
    "\n",
    "count = CountVectorizer()\n",
    "bag = count.fit_transform(twits)\n",
    "\n",
    "count.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from executing the preceding command, the vocabulary is stored in a Python dictionary that maps the unique words to integer indices. Next, let's print the feature vectors that we just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 2, 1, 1, 0, 0, 1, 0, 0, 1],\n",
       "       [1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each index position in the feature vectors corresponds to the integer values that are stored as dictionary items in the CountVectorizer vocabulary. For example, the first feature at index position 0 resembles the count of the word 'about' , which only occurs in the last document, and the word 'is' , at index position 7, occurs in all three twits (two times in the second twit). These values in the feature vectors are also called the **raw term frequencies**: `tf(t,d )` —the number of times a term `t` occurs in a document `d`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How relevant are words? Term frequency-inverse document frequency\n",
    "\n",
    "We could use these raw term frequencies to score the words in our algorithm. There is a problem though: If a word is very frequent in _all_ documents, then it probably doesn't carry a lot of information. In order to tacke this problem we can use **term frequency-inverse document frequency**, which will reduce the score the more frequent the word is accross all twits. It is calculated like this:\n",
    "\n",
    "\\begin{equation*}\n",
    "tf-idf(t,d) = tf(t,d) ~ idf(t,d)\n",
    "\\end{equation*}\n",
    "\n",
    "_tf(t,d)_ is the raw term frequency descrived above. _idf(t,d)_ is the inverse document frequency, than can be calculated as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\log \\frac{n_d}{1+df\\left(d,t\\right)}\n",
    "\\end{equation*}\n",
    "\n",
    "where `n` is the total number of documents and _df(t,d)_ is the number of documents where the term `t` appears. \n",
    "\n",
    "The `1` addition in the denominator is just to avoid zero term for terms that appear in all documents. Ans the `log` ensures that low frequency term don't get too much weight.\n",
    "\n",
    "Fortunately for us `scikit-learn` does all those calculations for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.72, 0.  , 0.  , 0.  , 0.  , 0.43, 0.  , 0.  , 0.  ,\n",
       "        0.  , 0.  , 0.55, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.4 , 0.  , 0.  , 0.  , 0.47, 0.4 , 0.4 , 0.  ,\n",
       "        0.  , 0.4 , 0.  , 0.  , 0.4 ],\n",
       "       [0.33, 0.33, 0.  , 0.  , 0.33, 0.33, 0.33, 0.2 , 0.  , 0.  , 0.33,\n",
       "        0.33, 0.  , 0.25, 0.33, 0.  ]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True,\n",
    "                         norm='l2',\n",
    "                         smooth_idf=True)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Feed the tf-idf transformer with our previously created Bag of Words\n",
    "tfidf.fit_transform(bag).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, words that appear in all documents like _is_ (with 0.47 ), get a lower score than others that don't appear in all documents, like _amazing_ (with 0.72).\n",
    "\n",
    "Note also that `norm='l2'` parameter: This is an important one, and what is doing is normalize the tf-idfs so that they're all in the same scale and thus work better with Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data clean up (yay...)\n",
    "\n",
    "### Removing stop words\n",
    "\n",
    "Now that we know how to format and score our input, we can start doing the analysis! Can we?... Well, we _can_, but let's look at our **real** vocabulary. Specifically, the most common words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('add', 808),\n",
       " ('minut', 658),\n",
       " ('heat', 432),\n",
       " ('cook', 432),\n",
       " ('1', 417),\n",
       " ('2', 335),\n",
       " ('stir', 334),\n",
       " ('bake', 333),\n",
       " ('water', 321),\n",
       " ('mix', 314),\n",
       " ('place', 313),\n",
       " ('pan', 312),\n",
       " ('serv', 306),\n",
       " ('bowl', 301),\n",
       " ('mixtur', 296),\n",
       " ('oil', 287),\n",
       " ('oven', 259),\n",
       " ('remov', 255),\n",
       " ('salt', 247),\n",
       " ('butter', 234)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab = Counter()\n",
    "for twit in train.recipedetails:\n",
    "    for word in twit.split(' '):\n",
    "        vocab[word] += 1\n",
    "\n",
    "vocab.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the most common words are meaningless in terms of sentiment: _I, to, the, and_... they don't give any information on positiveness or negativeness. They're basically **noise** that can most probably be eliminated. Let's see the whole distribution to convince ourselves of this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1276\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1276\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1276\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1276' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.2.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1276\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1276\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1276\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1276' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.2.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.2.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.2.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.2.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1276\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from bokeh.plotting import figure, show, output_file\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"73e13761-edb5-4428-bb1a-fb6554da423b\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"e2bf85d6-ec4e-4059-8874-82c9c5c0f46a\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1288\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"1293\",\"type\":\"LinearAxis\"}],\"renderers\":[{\"id\":\"1288\",\"type\":\"LinearAxis\"},{\"id\":\"1292\",\"type\":\"Grid\"},{\"id\":\"1293\",\"type\":\"LinearAxis\"},{\"id\":\"1297\",\"type\":\"Grid\"},{\"id\":\"1310\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"1277\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"1302\",\"type\":\"Toolbar\"},\"toolbar_location\":\"above\",\"x_range\":{\"id\":\"1280\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"1284\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"1282\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"1286\",\"type\":\"LinearScale\"}},\"id\":\"1278\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"1284\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1339\",\"type\":\"Selection\"},{\"attributes\":{},\"id\":\"1286\",\"type\":\"LinearScale\"},{\"attributes\":{\"formatter\":{\"id\":\"1334\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"1278\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1289\",\"type\":\"BasicTicker\"}},\"id\":\"1288\",\"type\":\"LinearAxis\"},{\"attributes\":{\"bottom\":{\"value\":0},\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"left\":{\"field\":\"left\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"right\":{\"field\":\"right\"},\"top\":{\"field\":\"top\"}},\"id\":\"1309\",\"type\":\"Quad\"},{\"attributes\":{\"plot\":{\"id\":\"1278\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1289\",\"type\":\"BasicTicker\"}},\"id\":\"1292\",\"type\":\"Grid\"},{\"attributes\":{\"formatter\":{\"id\":\"1336\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"1278\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1294\",\"type\":\"BasicTicker\"}},\"id\":\"1293\",\"type\":\"LinearAxis\"},{\"attributes\":{\"callback\":null},\"id\":\"1280\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1294\",\"type\":\"BasicTicker\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"1278\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1294\",\"type\":\"BasicTicker\"}},\"id\":\"1297\",\"type\":\"Grid\"},{\"attributes\":{\"data_source\":{\"id\":\"1307\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1308\",\"type\":\"Quad\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1309\",\"type\":\"Quad\"},\"selection_glyph\":null,\"view\":{\"id\":\"1311\",\"type\":\"CDSView\"}},\"id\":\"1310\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null,\"data\":{\"left\":{\"__ndarray__\":\"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\",\"dtype\":\"float64\",\"shape\":[500]},\"right\":{\"__ndarray__\":\"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\",\"dtype\":\"float64\",\"shape\":[500]},\"top\":{\"__ndarray__\":\"bMwvKMaqP0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAnC74FQ/oI0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADEKfcjw4AWQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAejB0o+8NQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAI2jVQdfE/z8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADajKh0TvT+PwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwQ8hyouH6PwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEjWTto8U+0/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJKzB29KvPI/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkVxCAZU8j8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAG1FjtbkLeU/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANqNbtiQQOk/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJKzB29KvOI/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAG3WFD5/n+c/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC2IYEHsErgPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA2440PNOM4z8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABI1k7aPFPdPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACRqYQlcXeQ/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAkrJBCwhw2D8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAeIYEHsErQPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADfVTto8U80/AAAAAAAAAAAAAAAAAAAAANuONDzTjNM/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAASNZO2jxTzT8AAAAAAAAAAAAAAAAAAAAASNZO2jxT3T8AAAAAAAAAAAAAAAAAAAAA//vncPbO1j8AAAAAAAAAAAAAAAAAAAAA2440PNOMwz8AAAAAAAAAAAAAAAAAAAAAMWibpRkRyj8AAAAAAAAAAP/753D2zsY/AAAAAAAAAAAAAAAAAAAAACRpm6UZEco/AAAAAAAAAAAAAAAAAAAAADfVTto8U80/AAAAAAAAAAAkaZulGRHKPwAAAAAAAAAA2440PNOMwz8AAAAAAAAAAAAAAAAAAAAAMWibpRkRuj8AAAAAAAAAALYhgQewSsA/AAAAAAAAAAAq++dw9s7GPwAAAAAAAAAAJGmbpRkRyj8AAAAAAAAAACSONDzTjMM/AAAAAAAAAAAkaZulGRG6PwAAAAAAAAAAJGmbpRkRuj8AAAAAAAAAAP/753D2zsY/AAAAAAAAAABI1k7aPFPNPwAAAAAAAAAAJGmbpRkRuj8kjjQ804zDPwAAAAAAAAAA2440PNOMsz8AAAAAAAAAACSONDzTjMM/tiGBB7BKwD8AAAAAAAAAALYhgQewSsA/AAAAAAAAAADbjjQ804yzPyRpm6UZEZo/AAAAAAAAAAD/++dw9s7GP7YhgQewSsA/AAAAAAAAAAAxaJulGRGaPyRpm6UZEbo/2440PNOMsz8AAAAAAAAAACRpm6UZEZo/2440PNOMsz8kaZulGRGqPwAAAAAAAAAAJGmbpRkRqj8AAAAAAAAAACRpm6UZEao/AAAAAAAAAAAkaZulGRGqP7YhgQewSsA/JGmbpRkRmj8kaZulGRGqP9uONDzTjLM/PWebpRkRqj8AAAAAAAAAACRpm6UZEZo/JGmbpRkRmj8kaZulGRGaPwAAAAAAAAAAtiGBB7BKwD8AAAAAAAAAAAAAAAAAAAAAJGmbpRkRmj8kaZulGRGaPyRpm6UZEao/JGmbpRkRmj8kaZulGRGqPyRpm6UZEao/PWebpRkRmj8kaZulGRGaPyRpm6UZEZo/JGmbpRkRuj8kaZulGRGaPyRpm6UZEZo/AAAAAAAAAADbjjQ804yzPwAAAAAAAAAAJGmbpRkRmj8kaZulGRGqPyRpm6UZEao/2440PNOMsz/bjjQ804yzPyRpm6UZEZo/PWebpRkRqj8kaZulGRGaPwAAAAAAAAAAAAAAAAAAAAAkaZulGRGqPyRpm6UZEao/AAAAAAAAAAAkaZulGRG6PwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACRpm6UZEao/JGmbpRkRqj8AAAAAAAAAACRpm6UZEao/PWebpRkRmj8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACRpm6UZEZo/2440PNOMsz8kaZulGRGqPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACRpm6UZEZo/2440PNOMsz8kaZulGRGaPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA2440PNOMsz8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAkaZulGRGaPwAAAAAAAAAAAAAAAAAAAAAkaZulGRGqPwAAAAAAAAAAJGmbpRkRuj8kaZulGRGqPyRpm6UZEao/AAAAAAAAAAAkaZulGRGaPwAAAAAAAAAAJGmbpRkRmj8AAAAAAAAAAAAAAAAAAAAAJGmbpRkRqj8kaZulGRG6PwAAAAAAAAAAJGmbpRkRqj8AAAAAAAAAACRpm6UZEZo/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPWebpRkRmj8AAAAAAAAAACRpm6UZEZo/AAAAAAAAAAAAAAAAAAAAACRpm6UZEZo/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJGmbpRkRmj8AAAAAAAAAACRpm6UZEZo/AAAAAAAAAAAkaZulGRGaPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACRpm6UZEZo/AAAAAAAAAAAkaZulGRGaPwAAAAAAAAAAJGmbpRkRmj8kaZulGRGaPyRpm6UZEZo/JGmbpRkRqj8AAAAAAAAAACRpm6UZEZo/AAAAAAAAAAAkaZulGRGaPyRpm6UZEao/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJGmbpRkRmj8AAAAAAAAAAAAAAAAAAAAAJGmbpRkRqj8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAkaZulGRGaPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAkaZulGRGaPw==\",\"dtype\":\"float64\",\"shape\":[500]}},\"selected\":{\"id\":\"1339\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1338\",\"type\":\"UnionRenderers\"}},\"id\":\"1307\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1300\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"1298\",\"type\":\"PanTool\"},{\"attributes\":{\"callback\":null},\"id\":\"1282\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1299\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"bottom\":{\"value\":0},\"fill_color\":{\"value\":\"#1f77b4\"},\"left\":{\"field\":\"left\"},\"line_color\":{\"value\":\"#555555\"},\"right\":{\"field\":\"right\"},\"top\":{\"field\":\"top\"}},\"id\":\"1308\",\"type\":\"Quad\"},{\"attributes\":{},\"id\":\"1301\",\"type\":\"SaveTool\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1298\",\"type\":\"PanTool\"},{\"id\":\"1299\",\"type\":\"WheelZoomTool\"},{\"id\":\"1300\",\"type\":\"ResetTool\"},{\"id\":\"1301\",\"type\":\"SaveTool\"}]},\"id\":\"1302\",\"type\":\"Toolbar\"},{\"attributes\":{\"source\":{\"id\":\"1307\",\"type\":\"ColumnDataSource\"}},\"id\":\"1311\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"1334\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1289\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1336\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"plot\":null,\"text\":\"Word distribution accross all twits\"},\"id\":\"1277\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1338\",\"type\":\"UnionRenderers\"}],\"root_ids\":[\"1278\"]},\"title\":\"Bokeh Application\",\"version\":\"1.0.2\"}};\n",
       "  var render_items = [{\"docid\":\"e2bf85d6-ec4e-4059-8874-82c9c5c0f46a\",\"roots\":{\"1278\":\"73e13761-edb5-4428-bb1a-fb6554da423b\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1278"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def plot_distribution(vocabulary):\n",
    "\n",
    "    hist, edges = np.histogram(list(map(lambda x:math.log(x[1]),vocabulary.most_common())), density=True, bins=500)\n",
    "\n",
    "    p = figure(tools=\"pan,wheel_zoom,reset,save\",\n",
    "               toolbar_location=\"above\",\n",
    "               title=\"Word distribution accross all twits\")\n",
    "    p.quad(top=hist, bottom=0, left=edges[:-1], right=edges[1:], line_color=\"#555555\", )\n",
    "    show(p)\n",
    "\n",
    "plot_distribution(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's clear now that a porcion of the words are overly represented. These kind of words are called _stop words_, and it is a common practice to remove them when doing text analysis. Let's do it and see the distribution again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ehosseiniasl/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('add', 808),\n",
       " ('minut', 658),\n",
       " ('heat', 432),\n",
       " ('cook', 432),\n",
       " ('1', 417),\n",
       " ('2', 335),\n",
       " ('stir', 334),\n",
       " ('bake', 333),\n",
       " ('water', 321),\n",
       " ('mix', 314),\n",
       " ('place', 313),\n",
       " ('pan', 312),\n",
       " ('serv', 306),\n",
       " ('bowl', 301),\n",
       " ('mixtur', 296),\n",
       " ('oil', 287),\n",
       " ('oven', 259),\n",
       " ('remov', 255),\n",
       " ('salt', 247),\n",
       " ('butter', 234)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "vocab_reduced = Counter()\n",
    "for w, c in vocab.items():\n",
    "    if not w in stop:\n",
    "        vocab_reduced[w]=c\n",
    "\n",
    "vocab_reduced.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks better, only in the 20 most common words we already see words that make sense: _good, love, really_... Let's see the distribution now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"cd740ad5-432d-493b-be88-e1c2bd565698\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"632e46e1-dd91-43ef-87d5-a2909760097e\":{\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1393\",\"type\":\"LinearAxis\"}],\"left\":[{\"id\":\"1398\",\"type\":\"LinearAxis\"}],\"renderers\":[{\"id\":\"1393\",\"type\":\"LinearAxis\"},{\"id\":\"1397\",\"type\":\"Grid\"},{\"id\":\"1398\",\"type\":\"LinearAxis\"},{\"id\":\"1402\",\"type\":\"Grid\"},{\"id\":\"1415\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"1382\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"1407\",\"type\":\"Toolbar\"},\"toolbar_location\":\"above\",\"x_range\":{\"id\":\"1385\",\"type\":\"DataRange1d\"},\"x_scale\":{\"id\":\"1389\",\"type\":\"LinearScale\"},\"y_range\":{\"id\":\"1387\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"1391\",\"type\":\"LinearScale\"}},\"id\":\"1383\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"1383\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1399\",\"type\":\"BasicTicker\"}},\"id\":\"1402\",\"type\":\"Grid\"},{\"attributes\":{\"plot\":null,\"text\":\"Word distribution accross all twits\"},\"id\":\"1382\",\"type\":\"Title\"},{\"attributes\":{\"bottom\":{\"value\":0},\"fill_color\":{\"value\":\"#1f77b4\"},\"left\":{\"field\":\"left\"},\"line_color\":{\"value\":\"#555555\"},\"right\":{\"field\":\"right\"},\"top\":{\"field\":\"top\"}},\"id\":\"1413\",\"type\":\"Quad\"},{\"attributes\":{},\"id\":\"1399\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1403\",\"type\":\"PanTool\"},{\"attributes\":{\"formatter\":{\"id\":\"1448\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"1383\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1399\",\"type\":\"BasicTicker\"}},\"id\":\"1398\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1405\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"1406\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"1451\",\"type\":\"Selection\"},{\"attributes\":{\"bottom\":{\"value\":0},\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"left\":{\"field\":\"left\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"right\":{\"field\":\"right\"},\"top\":{\"field\":\"top\"}},\"id\":\"1414\",\"type\":\"Quad\"},{\"attributes\":{\"callback\":null},\"id\":\"1385\",\"type\":\"DataRange1d\"},{\"attributes\":{\"callback\":null},\"id\":\"1387\",\"type\":\"DataRange1d\"},{\"attributes\":{\"formatter\":{\"id\":\"1446\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"1383\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1394\",\"type\":\"BasicTicker\"}},\"id\":\"1393\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1391\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1394\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1404\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"1448\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"plot\":{\"id\":\"1383\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"1394\",\"type\":\"BasicTicker\"}},\"id\":\"1397\",\"type\":\"Grid\"},{\"attributes\":{\"data_source\":{\"id\":\"1412\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"1413\",\"type\":\"Quad\"},\"hover_glyph\":null,\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"1414\",\"type\":\"Quad\"},\"selection_glyph\":null,\"view\":{\"id\":\"1416\",\"type\":\"CDSView\"}},\"id\":\"1415\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"source\":{\"id\":\"1412\",\"type\":\"ColumnDataSource\"}},\"id\":\"1416\",\"type\":\"CDSView\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_multi\":null,\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"1403\",\"type\":\"PanTool\"},{\"id\":\"1404\",\"type\":\"WheelZoomTool\"},{\"id\":\"1405\",\"type\":\"ResetTool\"},{\"id\":\"1406\",\"type\":\"SaveTool\"}]},\"id\":\"1407\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1446\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1389\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"1450\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"callback\":null,\"data\":{\"left\":{\"__ndarray__\":\"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\",\"dtype\":\"float64\",\"shape\":[500]},\"right\":{\"__ndarray__\":\"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\",\"dtype\":\"float64\",\"shape\":[500]},\"top\":{\"__ndarray__\":\"bMwvKMaqP0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAnC74FQ/oI0AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADEKfcjw4AWQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADAejB0o+8NQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAI2jVQdfE/z8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADajKh0TvT+PwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADwQ8hyouH6PwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAEjWTto8U+0/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJKzB29KvPI/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQkVxCAZU8j8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAG1FjtbkLeU/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAANqNbtiQQOk/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJKzB29KvOI/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAG3WFD5/n+c/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAC2IYEHsErgPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA2440PNOM4z8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABI1k7aPFPdPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACRqYQlcXeQ/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAkrJBCwhw2D8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAeIYEHsErQPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADfVTto8U80/AAAAAAAAAAAAAAAAAAAAANuONDzTjNM/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAASNZO2jxTzT8AAAAAAAAAAAAAAAAAAAAASNZO2jxT3T8AAAAAAAAAAAAAAAAAAAAA//vncPbO1j8AAAAAAAAAAAAAAAAAAAAA2440PNOMwz8AAAAAAAAAAAAAAAAAAAAAMWibpRkRyj8AAAAAAAAAAP/753D2zsY/AAAAAAAAAAAAAAAAAAAAACRpm6UZEco/AAAAAAAAAAAAAAAAAAAAADfVTto8U80/AAAAAAAAAAAkaZulGRHKPwAAAAAAAAAA2440PNOMwz8AAAAAAAAAAAAAAAAAAAAAMWibpRkRuj8AAAAAAAAAALYhgQewSsA/AAAAAAAAAAAq++dw9s7GPwAAAAAAAAAAJGmbpRkRyj8AAAAAAAAAACSONDzTjMM/AAAAAAAAAAAkaZulGRG6PwAAAAAAAAAAJGmbpRkRuj8AAAAAAAAAAP/753D2zsY/AAAAAAAAAABI1k7aPFPNPwAAAAAAAAAAJGmbpRkRuj8kjjQ804zDPwAAAAAAAAAA2440PNOMsz8AAAAAAAAAACSONDzTjMM/tiGBB7BKwD8AAAAAAAAAALYhgQewSsA/AAAAAAAAAADbjjQ804yzPyRpm6UZEZo/AAAAAAAAAAD/++dw9s7GP7YhgQewSsA/AAAAAAAAAAAxaJulGRGaPyRpm6UZEbo/2440PNOMsz8AAAAAAAAAACRpm6UZEZo/2440PNOMsz8kaZulGRGqPwAAAAAAAAAAJGmbpRkRqj8AAAAAAAAAACRpm6UZEao/AAAAAAAAAAAkaZulGRGqP7YhgQewSsA/JGmbpRkRmj8kaZulGRGqP9uONDzTjLM/PWebpRkRqj8AAAAAAAAAACRpm6UZEZo/JGmbpRkRmj8kaZulGRGaPwAAAAAAAAAAtiGBB7BKwD8AAAAAAAAAAAAAAAAAAAAAJGmbpRkRmj8kaZulGRGaPyRpm6UZEao/JGmbpRkRmj8kaZulGRGqPyRpm6UZEao/PWebpRkRmj8kaZulGRGaPyRpm6UZEZo/JGmbpRkRuj8kaZulGRGaPyRpm6UZEZo/AAAAAAAAAADbjjQ804yzPwAAAAAAAAAAJGmbpRkRmj8kaZulGRGqPyRpm6UZEao/2440PNOMsz/bjjQ804yzPyRpm6UZEZo/PWebpRkRqj8kaZulGRGaPwAAAAAAAAAAAAAAAAAAAAAkaZulGRGqPyRpm6UZEao/AAAAAAAAAAAkaZulGRG6PwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACRpm6UZEao/JGmbpRkRqj8AAAAAAAAAACRpm6UZEao/PWebpRkRmj8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACRpm6UZEZo/2440PNOMsz8kaZulGRGqPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACRpm6UZEZo/2440PNOMsz8kaZulGRGaPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA2440PNOMsz8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAkaZulGRGaPwAAAAAAAAAAAAAAAAAAAAAkaZulGRGqPwAAAAAAAAAAJGmbpRkRuj8kaZulGRGqPyRpm6UZEao/AAAAAAAAAAAkaZulGRGaPwAAAAAAAAAAJGmbpRkRmj8AAAAAAAAAAAAAAAAAAAAAJGmbpRkRqj8kaZulGRG6PwAAAAAAAAAAJGmbpRkRqj8AAAAAAAAAACRpm6UZEZo/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPWebpRkRmj8AAAAAAAAAACRpm6UZEZo/AAAAAAAAAAAAAAAAAAAAACRpm6UZEZo/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJGmbpRkRmj8AAAAAAAAAACRpm6UZEZo/AAAAAAAAAAAkaZulGRGaPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACRpm6UZEZo/AAAAAAAAAAAkaZulGRGaPwAAAAAAAAAAJGmbpRkRmj8kaZulGRGaPyRpm6UZEZo/JGmbpRkRqj8AAAAAAAAAACRpm6UZEZo/AAAAAAAAAAAkaZulGRGaPyRpm6UZEao/AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJGmbpRkRmj8AAAAAAAAAAAAAAAAAAAAAJGmbpRkRqj8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAkaZulGRGaPwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAkaZulGRGaPw==\",\"dtype\":\"float64\",\"shape\":[500]}},\"selected\":{\"id\":\"1451\",\"type\":\"Selection\"},\"selection_policy\":{\"id\":\"1450\",\"type\":\"UnionRenderers\"}},\"id\":\"1412\",\"type\":\"ColumnDataSource\"}],\"root_ids\":[\"1383\"]},\"title\":\"Bokeh Application\",\"version\":\"1.0.2\"}};\n",
       "  var render_items = [{\"docid\":\"632e46e1-dd91-43ef-87d5-a2909760097e\",\"roots\":{\"1383\":\"cd740ad5-432d-493b-be88-e1c2bd565698\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1383"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_distribution(vocab_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing special characters and \"trash\"\n",
    "\n",
    "We still se a very uneaven distribution. If you look closer, you'll see that we're also taking into consideration punctuation signs ('-', ',', etc) and other html tags like `&amp`. We can definitely remove them for the sentiment analysis, but we will try to keep the emoticons, since those _do_ have a sentiment load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this twit man is nice :)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocessor(text):\n",
    "    \"\"\" Return a cleaned version of text\n",
    "    \"\"\"\n",
    "    # Remove HTML markup\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    # Save emoticons for later appending\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    # Remove any non-word character and append the emoticons,\n",
    "    # removing the nose character for standarization. Convert to lower case\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(preprocessor('This!! twit man :) is <b>nice</b>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are almost ready! There is another trick we can use to reduce our vocabulary and consolidate words. If you think about it, words like: love, loving, etc. _Could_ express the same positivity. If that was the case, we would be  having two words in our vocabulary when we could have only one: lov. This process of reducing a word to its root is called **steaming**.\n",
    "\n",
    "We also need a _tokenizer_ to break down our twits in individual words. We will implement two tokenizers, a regular one and one that does steaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'there,', 'I', 'am', 'loving', 'this,', 'like', 'with', 'a', 'lot', 'of', 'love']\n",
      "['Hi', 'there,', 'I', 'am', 'love', 'this,', 'like', 'with', 'a', 'lot', 'of', 'love']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "print(tokenizer('Hi there, I am loving this, like with a lot of love'))\n",
    "print(tokenizer_porter('Hi there, I am loving this, like with a lot of love'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Logistic Regression\n",
    "\n",
    "We are finally ready to train our algorythm. We need to choose the best hyperparameters like the _learning rate_ or _regularization strength_. We also would like to know if our algorithm performs better steaming words or not, or removing html or not, etc...\n",
    "\n",
    "To take these decisions methodically, we can use a Grid Search. Grid search is a method of training an algorythm with different variations of parameters to latter select the best combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the dataset in train and test\n",
    "X = train['recipedetails']\n",
    "y = train['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code line above, `stratify` will create a train set with the same class balance than the original set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'vect__preprocessor': [None, preprocessor],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              {'vect__ngram_range': [(1, 1)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "               'vect__preprocessor': [None, preprocessor],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              ]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression(random_state=0))])\n",
    "\n",
    "#lr_tfidf = Pipeline([('vect', tfidf),\n",
    "#                     ('clf', BaggingRegressor(LogisticRegression(random_state=0), n_estimators=50, bootstrap=True))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    3.6s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:   35.5s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:  1.7min finished\n",
      "/home/ehosseiniasl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,...e, penalty='l2', random_state=0, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid=[{'vect__ngram_range': [(1, 1)], 'vect__stop_words': [['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's...se_idf': [False], 'vect__norm': [None], 'clf__penalty': ['l1', 'l2'], 'clf__C': [1.0, 10.0, 100.0]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note: This may take a long while to execute, like... 1 or 2 hours\n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 100.0, 'clf__penalty': 'l1', 'vect__ngram_range': (1, 1), 'vect__preprocessor': None, 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"], 'vect__tokenizer': <function tokenizer at 0x7f88486fa400>}\n",
      "Best accuracy: 0.928\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter set: ' + str(gs_lr_tfidf.best_params_))\n",
    "print('Best accuracy: %.3f' % gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the set of parameters that best results give us are:\n",
    "\n",
    "* A regularization strength of `1.0` using `l2` regularization\n",
    "* Using our `preprocessor` (removing html, keeping emoticons, etc) _does_ improve the performance\n",
    "* Surprisingly, removing stop words does not improve accuracy\n",
    "* word steming doesn't seem to help either\n",
    "\n",
    "As youcan see, sometimes intuition may lead to wrong decisions, and it's important to _test_ all our assumptions. \n",
    "\n",
    "Let's see what's our best accuracy then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in test: 0.946\n"
     ]
    }
   ],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print('Accuracy in test: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    2.1s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:   20.5s\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:   23.4s finished\n",
      "/home/ehosseiniasl/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', \"it'\", 'onc', 'onli', 'ourselv', \"she'\", \"should'v\", 'themselv', 'thi', 'veri', 'wa', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/home/ehosseiniasl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in test: 0.554\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed:   13.7s\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:   21.9s finished\n",
      "/home/ehosseiniasl/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', \"it'\", 'onc', 'onli', 'ourselv', \"she'\", \"should'v\", 'themselv', 'thi', 'veri', 'wa', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/home/ehosseiniasl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in test: 0.554\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed:   13.7s\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:   22.0s finished\n",
      "/home/ehosseiniasl/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', \"it'\", 'onc', 'onli', 'ourselv', \"she'\", \"should'v\", 'themselv', 'thi', 'veri', 'wa', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/home/ehosseiniasl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in test: 0.554\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 303 tasks      | elapsed:   13.9s\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:   21.9s finished\n",
      "/home/ehosseiniasl/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', \"it'\", 'onc', 'onli', 'ourselv', \"she'\", \"should'v\", 'themselv', 'thi', 'veri', 'wa', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/home/ehosseiniasl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in test: 0.554\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done 307 tasks      | elapsed:   14.0s\n",
      "[Parallel(n_jobs=-1)]: Done 480 out of 480 | elapsed:   22.0s finished\n",
      "/home/ehosseiniasl/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', \"it'\", 'onc', 'onli', 'ourselv', \"she'\", \"should'v\", 'themselv', 'thi', 'veri', 'wa', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy in test: 0.554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ehosseiniasl/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    gs_lr_tfidf.fit(X_train, y_train)\n",
    "    clf = gs_lr_tfidf.best_estimator_\n",
    "    print('Accuracy in test: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we would like to use the classifier in another place, or just not train it again and again everytime, we can save the model in a pickle file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "pickle.dump(clf, open(os.path.join('data', 'logisticRegression.pkl'), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's run some tests :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is really bad, I don't like it at all --> no\n",
      "I love this! --> no\n",
      ":) --> no\n",
      "I'm sad... :( --> no\n"
     ]
    }
   ],
   "source": [
    "twits = [\n",
    "    \"This is really bad, I don't like it at all\",\n",
    "    \"I love this!\",\n",
    "    \":)\",\n",
    "    \"I'm sad... :(\"\n",
    "]\n",
    "\n",
    "preds = clf.predict(twits)\n",
    "\n",
    "for i in range(len(twits)):\n",
    "    print(f'{twits[i]} --> {preds[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And you're done! I hope you liked this!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
